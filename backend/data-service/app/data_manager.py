#!/usr/bin/env python3
"""
æ•°æ®ç®¡ç†å™¨ - æ™ºèƒ½æ•°æ®è·å–å’Œç¼“å­˜ç­–ç•¥
æ”¯æŒå¤šæ•°æ®æºä¼˜å…ˆçº§ã€æœ¬åœ°ç¼“å­˜ã€å®šæ—¶æ›´æ–°
"""

import asyncio
import json
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum
from dataclasses import dataclass, asdict
from pymongo import MongoClient
import redis

# å¯¼å…¥å›½é™…åŒ–æ—¥å¿—
from backend.shared.i18n.logger import get_i18n_logger, get_compatible_logger
from backend.shared.i18n.config import SupportedLanguage

# åˆå§‹åŒ–å›½é™…åŒ–æ—¥å¿—
i18n_logger = get_i18n_logger("data-manager")
logger = get_compatible_logger("data-manager")  # å…¼å®¹ç°æœ‰ä»£ç 
# æ³¨æ„ï¼šæˆ‘ä»¬ç°åœ¨ä½¿ç”¨æ–°çš„æ•°æ®æºå·¥å‚ï¼Œä¸å†ä¾èµ–åŸé¡¹ç›®çš„ç»Ÿä¸€å·¥å…·
from .datasources.factory import get_data_source_factory, DataSourceFactory
from .datasources.base import MarketType, DataCategory

logger = logging.getLogger(__name__)

class DataSource(Enum):
    """æ•°æ®æºæšä¸¾"""
    TUSHARE = "tushare"
    AKSHARE = "akshare"
    BAOSTOCK = "baostock"
    YFINANCE = "yfinance"
    FINNHUB = "finnhub"

class DataType(Enum):
    """æ•°æ®ç±»å‹æšä¸¾"""
    STOCK_INFO = "stock_info"
    STOCK_DATA = "stock_data"
    FUNDAMENTALS = "fundamentals"
    NEWS = "news"

class DataPriority(Enum):
    """æ•°æ®ä¼˜å…ˆçº§"""
    HIGH = 1      # å®æ—¶æ•°æ®ï¼Œç¼“å­˜5åˆ†é’Ÿ
    MEDIUM = 2    # æ—¥å†…æ•°æ®ï¼Œç¼“å­˜1å°æ—¶
    LOW = 3       # å†å²æ•°æ®ï¼Œç¼“å­˜1å¤©

@dataclass
class DataConfig:
    """æ•°æ®é…ç½®"""
    data_type: DataType
    priority: DataPriority
    cache_duration: timedelta
    sources: List[DataSource]  # æŒ‰ä¼˜å…ˆçº§æ’åº
    update_interval: timedelta  # å®šæ—¶æ›´æ–°é—´éš”

@dataclass
class CachedData:
    """ç¼“å­˜æ•°æ®ç»“æ„"""
    symbol: str
    data_type: str
    data: Any
    source: str
    timestamp: datetime
    expires_at: datetime
    version: str = "1.0"

class DataManager:
    """æ™ºèƒ½æ•°æ®ç®¡ç†å™¨"""

    def __init__(self, mongodb_client: MongoClient, redis_client: redis.Redis, language: SupportedLanguage = None):
        self.mongodb = mongodb_client
        self.redis = redis_client
        self.db = mongodb_client.tradingagents

        # è®¾ç½®æ—¥å¿—è¯­è¨€
        if language:
            i18n_logger.set_language(language)

        # è®°å½•åˆå§‹åŒ–æ—¥å¿—
        i18n_logger.manager_initialized()

        # åˆå§‹åŒ–æ•°æ®æºå·¥å‚
        self.data_source_factory = get_data_source_factory()

        # æ•°æ®é…ç½®
        self.data_configs = {
            DataType.STOCK_INFO: DataConfig(
                data_type=DataType.STOCK_INFO,
                priority=DataPriority.LOW,
                cache_duration=timedelta(days=1),
                sources=[DataSource.TUSHARE, DataSource.AKSHARE, DataSource.YFINANCE],
                update_interval=timedelta(hours=24)
            ),
            DataType.STOCK_DATA: DataConfig(
                data_type=DataType.STOCK_DATA,
                priority=DataPriority.MEDIUM,
                cache_duration=timedelta(hours=1),
                sources=[DataSource.TUSHARE, DataSource.AKSHARE, DataSource.BAOSTOCK],
                update_interval=timedelta(minutes=15)
            ),
            DataType.FUNDAMENTALS: DataConfig(
                data_type=DataType.FUNDAMENTALS,
                priority=DataPriority.LOW,
                cache_duration=timedelta(hours=6),
                sources=[DataSource.TUSHARE, DataSource.AKSHARE],
                update_interval=timedelta(hours=6)
            ),
            DataType.NEWS: DataConfig(
                data_type=DataType.NEWS,
                priority=DataPriority.HIGH,
                cache_duration=timedelta(minutes=30),
                sources=[DataSource.FINNHUB, DataSource.AKSHARE],
                update_interval=timedelta(minutes=30)
            )
        }
        
        # åˆå§‹åŒ–é›†åˆç´¢å¼•
        self._init_indexes()

    def set_log_language(self, language: SupportedLanguage):
        """è®¾ç½®æ—¥å¿—è¯­è¨€"""
        i18n_logger.set_language(language)
        i18n_logger.info("log.data_manager.language_set", language=language.value)

    def _init_indexes(self):
        """åˆå§‹åŒ–æ•°æ®åº“ç´¢å¼•"""
        try:
            # ç¼“å­˜æ•°æ®ç´¢å¼•
            self.db.cached_data.create_index([
                ("symbol", 1), ("data_type", 1)
            ], unique=True)
            self.db.cached_data.create_index("expires_at")
            
            # å†å²æ•°æ®ç´¢å¼•
            self.db.stock_data.create_index([
                ("symbol", 1), ("date", -1)
            ])
            self.db.stock_info.create_index("symbol", unique=True)
            self.db.fundamentals.create_index([
                ("symbol", 1), ("date", -1)
            ])
            self.db.news.create_index([
                ("symbol", 1), ("timestamp", -1)
            ])
            
            logger.info("âœ… æ•°æ®åº“ç´¢å¼•åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“ç´¢å¼•åˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def get_data(self, symbol: str, data_type: DataType, **kwargs) -> Tuple[bool, Any]:
        """
        æ™ºèƒ½è·å–æ•°æ®
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data_type: æ•°æ®ç±»å‹
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            (success, data): æˆåŠŸæ ‡å¿—å’Œæ•°æ®
        """
        try:
            # è®°å½•å¼€å§‹å¤„ç†è¯·æ±‚
            i18n_logger.processing_request(symbol, data_type.value)
            start_time = time.time()

            # Debug: è®°å½•æŸ¥è¯¢å¼€å§‹
            i18n_logger.debug_query_start("data_request", symbol)

            # 1. æ£€æŸ¥ç¼“å­˜
            i18n_logger.debug_cache_check_start(symbol, data_type.value)
            cached_data = await self._get_cached_data(symbol, data_type)

            if cached_data and not self._is_expired(cached_data):
                i18n_logger.debug_cache_check_result("hit", symbol)
                i18n_logger.cache_hit(symbol, data_type.value)
                duration = int((time.time() - start_time) * 1000)
                i18n_logger.debug_query_end("data_request", duration)
                i18n_logger.request_completed(symbol, duration)
                return True, cached_data.data

            if cached_data:
                i18n_logger.debug_cache_check_result("expired", symbol)
                i18n_logger.cache_expired(symbol, data_type.value)
            else:
                i18n_logger.debug_cache_check_result("miss", symbol)
                i18n_logger.cache_miss(symbol, data_type.value)

            # 2. ä»æ•°æ®æºè·å–
            i18n_logger.debug("log.debug.data.fetch_start", symbol=symbol, data_type=data_type.value)
            success, fresh_data = await self._fetch_from_sources(symbol, data_type, **kwargs)

            if not success:
                i18n_logger.debug("log.debug.data.fetch_failed", symbol=symbol, data_type=data_type.value)
                # å¦‚æœè·å–å¤±è´¥ï¼Œè¿”å›è¿‡æœŸçš„ç¼“å­˜æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
                if cached_data:
                    i18n_logger.fallback_triggered("data_fetch_failed")
                    duration = int((time.time() - start_time) * 1000)
                    i18n_logger.debug_query_end("data_request", duration)
                    i18n_logger.request_completed(symbol, duration)
                    return True, cached_data.data
                i18n_logger.request_failed(symbol, "no_data_available")
                return False, None

            i18n_logger.debug("log.debug.data.fetch_success", symbol=symbol, data_type=data_type.value)

            # 3. ä¿å­˜åˆ°ç¼“å­˜å’Œæ•°æ®åº“
            i18n_logger.debug("log.debug.data.save_start", symbol=symbol, data_type=data_type.value)
            await self._save_data(symbol, data_type, fresh_data)
            i18n_logger.debug("log.debug.data.save_end", symbol=symbol, data_type=data_type.value)

            duration = int((time.time() - start_time) * 1000)
            i18n_logger.debug_query_end("data_request", duration)
            i18n_logger.data_fetched(symbol, "data_source_factory")
            i18n_logger.request_completed(symbol, duration)
            return True, fresh_data
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®è·å–å¤±è´¥: {symbol} - {data_type.value} - {e}")
            return False, None
    
    async def _get_cached_data(self, symbol: str, data_type: DataType) -> Optional[CachedData]:
        """è·å–ç¼“å­˜æ•°æ®"""
        try:
            # å…ˆæ£€æŸ¥ Redis
            redis_key = f"data:{symbol}:{data_type.value}"
            i18n_logger.debug("log.debug.data.redis_check", cache_key=redis_key)

            redis_data = self.redis.get(redis_key)
            if redis_data:
                i18n_logger.debug("log.debug.data.redis_hit", cache_key=redis_key)
                data_dict = json.loads(redis_data)

                # ç¡®ä¿datetimeå­—æ®µæ­£ç¡®è§£æ
                if 'timestamp' in data_dict and isinstance(data_dict['timestamp'], str):
                    data_dict['timestamp'] = datetime.fromisoformat(data_dict['timestamp'].replace('Z', '+00:00'))
                if 'expires_at' in data_dict and isinstance(data_dict['expires_at'], str):
                    data_dict['expires_at'] = datetime.fromisoformat(data_dict['expires_at'].replace('Z', '+00:00'))

                return CachedData(**data_dict)

            i18n_logger.debug("log.debug.data.redis_miss", cache_key=redis_key)

            # å†æ£€æŸ¥ MongoDB
            i18n_logger.debug("log.debug.data.mongo_check", symbol=symbol, data_type=data_type.value)
            doc = self.db.cached_data.find_one({
                "symbol": symbol,
                "data_type": data_type.value
            })

            if doc:
                i18n_logger.debug("log.debug.data.mongo_hit", symbol=symbol, data_type=data_type.value)
                doc.pop("_id", None)
                cached_data = CachedData(**doc)

                # åŒæ­¥åˆ° Redis
                ttl = int(self.data_configs[data_type].cache_duration.total_seconds())
                i18n_logger.debug_cache_save_start(symbol, data_type.value)
                self.redis.setex(
                    redis_key,
                    ttl,
                    json.dumps(asdict(cached_data), default=str)
                )
                i18n_logger.debug_cache_save_end(symbol, ttl)
                
                return cached_data
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _is_expired(self, cached_data: CachedData) -> bool:
        """æ£€æŸ¥æ•°æ®æ˜¯å¦è¿‡æœŸ"""
        return datetime.now() > cached_data.expires_at
    
    async def _fetch_from_sources(self, symbol: str, data_type: DataType, **kwargs) -> Tuple[bool, Any]:
        """ä»æ•°æ®æºè·å–æ•°æ®"""
        try:
            # æ£€æµ‹å¸‚åœºç±»å‹
            market_type = self.data_source_factory.detect_market_type(symbol)
            logger.info(f"ğŸ” æ£€æµ‹åˆ°å¸‚åœºç±»å‹: {market_type.value} for {symbol}")

            # ä½¿ç”¨æ•°æ®æºå·¥å‚è·å–æ•°æ®
            if data_type == DataType.STOCK_INFO:
                data = await self.data_source_factory.get_stock_info(symbol, market_type)
                category = DataCategory.BASIC_INFO

            elif data_type == DataType.STOCK_DATA:
                start_date = kwargs.get('start_date')
                end_date = kwargs.get('end_date')
                data = await self.data_source_factory.get_stock_data(symbol, market_type, start_date, end_date)
                category = DataCategory.PRICE_DATA

            elif data_type == DataType.FUNDAMENTALS:
                start_date = kwargs.get('start_date')
                end_date = kwargs.get('end_date')
                data = await self.data_source_factory.get_fundamentals(symbol, market_type, start_date, end_date)
                category = DataCategory.FUNDAMENTALS

            elif data_type == DataType.NEWS:
                start_date = kwargs.get('start_date', (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d'))
                end_date = kwargs.get('end_date', datetime.now().strftime('%Y-%m-%d'))
                data = await self.data_source_factory.get_news(symbol, market_type, start_date, end_date)
                category = DataCategory.NEWS

            else:
                logger.error(f"âŒ ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type.value}")
                return False, None

            if data:
                logger.info(f"âœ… æ•°æ®æºå·¥å‚è·å–æˆåŠŸ: {symbol} - {data_type.value}")
                return True, {
                    "source": "data_source_factory",
                    "market_type": market_type.value,
                    "category": category.value,
                    "data": data,
                    "timestamp": datetime.now().isoformat()
                }
            else:
                logger.warning(f"âš ï¸ æ•°æ®æºå·¥å‚æœªè·å–åˆ°æ•°æ®: {symbol} - {data_type.value}")

                # é™çº§åˆ°åŸæœ‰çš„ç»Ÿä¸€å·¥å…·
                return await self._fetch_from_legacy_tools(symbol, data_type, **kwargs)

        except Exception as e:
            logger.error(f"âŒ æ•°æ®æºå·¥å‚è·å–å¤±è´¥: {symbol} - {data_type.value} - {e}")
            # é™çº§åˆ°åŸæœ‰çš„ç»Ÿä¸€å·¥å…·
            return await self._fetch_from_legacy_tools(symbol, data_type, **kwargs)

    async def _fetch_from_legacy_tools(self, symbol: str, data_type: DataType, **kwargs) -> Tuple[bool, Any]:
        """ä»åŸæœ‰ç»Ÿä¸€å·¥å…·è·å–æ•°æ®ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        try:
            logger.info(f"ğŸ”„ ä½¿ç”¨åŸæœ‰ç»Ÿä¸€å·¥å…·è·å–æ•°æ®: {symbol} - {data_type.value}")

            # æ ¹æ®æ•°æ®ç±»å‹è°ƒç”¨ç›¸åº”çš„è·å–å‡½æ•°
            if data_type == DataType.STOCK_INFO:
                data = get_stock_info(symbol)
            elif data_type == DataType.STOCK_DATA:
                start_date = kwargs.get('start_date')
                end_date = kwargs.get('end_date')
                data = get_stock_data(symbol, start_date, end_date)
            elif data_type == DataType.FUNDAMENTALS:
                start_date = kwargs.get('start_date')
                end_date = kwargs.get('end_date')
                curr_date = kwargs.get('curr_date')
                data = get_stock_fundamentals(symbol, start_date, end_date, curr_date)
            elif data_type == DataType.NEWS:
                curr_date = kwargs.get('curr_date', datetime.now().strftime('%Y-%m-%d'))
                hours_back = kwargs.get('hours_back', 24)
                data = get_stock_news(symbol, curr_date, hours_back)
            else:
                return False, None

            if data:
                logger.info(f"âœ… åŸæœ‰ç»Ÿä¸€å·¥å…·è·å–æˆåŠŸ: {symbol} - {data_type.value}")
                return True, {
                    "source": "legacy_unified_tools",
                    "data": data,
                    "timestamp": datetime.now().isoformat()
                }
            else:
                logger.error(f"âŒ åŸæœ‰ç»Ÿä¸€å·¥å…·ä¹Ÿæ— æ³•è·å–æ•°æ®: {symbol} - {data_type.value}")
                return False, None

        except Exception as e:
            logger.error(f"âŒ åŸæœ‰ç»Ÿä¸€å·¥å…·è·å–å¤±è´¥: {symbol} - {data_type.value} - {e}")
            return False, None
    
    async def _save_data(self, symbol: str, data_type: DataType, data: Any):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜å’Œæ•°æ®åº“"""
        try:
            config = self.data_configs[data_type]
            now = datetime.now()
            expires_at = now + config.cache_duration

            # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
            if isinstance(data, dict):
                source = data.get("source", "unknown")
                actual_data = data.get("data", data)
            else:
                source = "unknown"
                actual_data = data

            cached_data = CachedData(
                symbol=symbol,
                data_type=data_type.value,
                data=data,  # ä¿å­˜å®Œæ•´çš„æ•°æ®ç»“æ„
                source=source,
                timestamp=now,
                expires_at=expires_at
            )

            # ä¿å­˜åˆ° MongoDB ç¼“å­˜è¡¨
            try:
                self.db.cached_data.replace_one(
                    {"symbol": symbol, "data_type": data_type.value},
                    asdict(cached_data),
                    upsert=True
                )
                logger.info(f"âœ… MongoDB ç¼“å­˜ä¿å­˜æˆåŠŸ: {symbol} - {data_type.value}")
            except Exception as e:
                logger.error(f"âŒ MongoDB ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

            # ä¿å­˜åˆ° Redis
            try:
                redis_key = f"data:{symbol}:{data_type.value}"
                self.redis.setex(
                    redis_key,
                    int(config.cache_duration.total_seconds()),
                    json.dumps(asdict(cached_data), default=str)
                )
                logger.info(f"âœ… Redis ç¼“å­˜ä¿å­˜æˆåŠŸ: {symbol} - {data_type.value}")
            except Exception as e:
                logger.error(f"âŒ Redis ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

            # ä¿å­˜åˆ°å†å²æ•°æ®è¡¨
            await self._save_historical_data(symbol, data_type, data)

            logger.info(f"âœ… æ•°æ®ä¿å­˜å®Œæˆ: {symbol} - {data_type.value}")

        except Exception as e:
            logger.error(f"âŒ æ•°æ®ä¿å­˜å¤±è´¥: {symbol} - {data_type.value} - {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
    
    async def _save_historical_data(self, symbol: str, data_type: DataType, data: Any):
        """ä¿å­˜å†å²æ•°æ®åˆ°ä¸“é—¨çš„å†å²æ•°æ®è¡¨"""
        try:
            # æå–å®é™…æ•°æ®
            if isinstance(data, dict):
                actual_data = data.get("data", data)
                source = data.get("source", "unknown")
                market_type = data.get("market_type", "unknown")
            else:
                actual_data = data
                source = "unknown"
                market_type = "unknown"

            now = datetime.now()

            if data_type == DataType.STOCK_INFO:
                # è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
                try:
                    self.db.stock_info.replace_one(
                        {"symbol": symbol},
                        {
                            "symbol": symbol,
                            "data": actual_data,
                            "source": source,
                            "market_type": market_type,
                            "updated_at": now,
                            "created_at": now
                        },
                        upsert=True
                    )
                    logger.info(f"âœ… è‚¡ç¥¨ä¿¡æ¯å†å²æ•°æ®ä¿å­˜æˆåŠŸ: {symbol}")
                except Exception as e:
                    logger.error(f"âŒ è‚¡ç¥¨ä¿¡æ¯å†å²æ•°æ®ä¿å­˜å¤±è´¥: {e}")

            elif data_type == DataType.STOCK_DATA:
                # è‚¡ç¥¨ä»·æ ¼æ•°æ®æŒ‰æ—¥æœŸå­˜å‚¨
                try:
                    if isinstance(actual_data, list):
                        saved_count = 0
                        for record in actual_data:
                            if isinstance(record, dict) and record.get("date"):
                                self.db.stock_data.replace_one(
                                    {"symbol": symbol, "date": record.get("date")},
                                    {
                                        "symbol": symbol,
                                        "date": record.get("date"),
                                        "open": record.get("open"),
                                        "high": record.get("high"),
                                        "low": record.get("low"),
                                        "close": record.get("close"),
                                        "volume": record.get("volume"),
                                        "amount": record.get("amount"),
                                        "source": source,
                                        "market_type": market_type,
                                        "updated_at": now,
                                        "created_at": now
                                    },
                                    upsert=True
                                )
                                saved_count += 1
                        logger.info(f"âœ… è‚¡ç¥¨ä»·æ ¼å†å²æ•°æ®ä¿å­˜æˆåŠŸ: {symbol}, {saved_count} æ¡è®°å½•")
                    else:
                        logger.warning(f"âš ï¸ è‚¡ç¥¨ä»·æ ¼æ•°æ®æ ¼å¼ä¸æ­£ç¡®: {symbol}")
                except Exception as e:
                    logger.error(f"âŒ è‚¡ç¥¨ä»·æ ¼å†å²æ•°æ®ä¿å­˜å¤±è´¥: {e}")

            elif data_type == DataType.FUNDAMENTALS:
                # åŸºæœ¬é¢æ•°æ®
                try:
                    report_date = actual_data.get("report_date") if isinstance(actual_data, dict) else now.strftime("%Y-%m-%d")
                    self.db.fundamentals.replace_one(
                        {"symbol": symbol, "report_date": report_date},
                        {
                            "symbol": symbol,
                            "report_date": report_date,
                            "data": actual_data,
                            "source": source,
                            "market_type": market_type,
                            "updated_at": now,
                            "created_at": now
                        },
                        upsert=True
                    )
                    logger.info(f"âœ… åŸºæœ¬é¢å†å²æ•°æ®ä¿å­˜æˆåŠŸ: {symbol}")
                except Exception as e:
                    logger.error(f"âŒ åŸºæœ¬é¢å†å²æ•°æ®ä¿å­˜å¤±è´¥: {e}")

            elif data_type == DataType.NEWS:
                # æ–°é—»æ•°æ®
                try:
                    if isinstance(actual_data, list):
                        saved_count = 0
                        for news_item in actual_data:
                            if isinstance(news_item, dict):
                                # ä½¿ç”¨æ–°é—»æ ‡é¢˜å’Œå‘å¸ƒæ—¶é—´ä½œä¸ºå”¯ä¸€æ ‡è¯†
                                title = news_item.get("title", "")
                                publish_time = news_item.get("publish_time", now.isoformat())

                                self.db.news.replace_one(
                                    {"symbol": symbol, "title": title, "publish_time": publish_time},
                                    {
                                        "symbol": symbol,
                                        "title": title,
                                        "content": news_item.get("content", ""),
                                        "publish_time": publish_time,
                                        "source": news_item.get("source", source),
                                        "url": news_item.get("url", ""),
                                        "market_type": market_type,
                                        "updated_at": now,
                                        "created_at": now
                                    },
                                    upsert=True
                                )
                                saved_count += 1
                        logger.info(f"âœ… æ–°é—»å†å²æ•°æ®ä¿å­˜æˆåŠŸ: {symbol}, {saved_count} æ¡è®°å½•")
                    else:
                        logger.warning(f"âš ï¸ æ–°é—»æ•°æ®æ ¼å¼ä¸æ­£ç¡®: {symbol}")
                except Exception as e:
                    logger.error(f"âŒ æ–°é—»å†å²æ•°æ®ä¿å­˜å¤±è´¥: {e}")

        except Exception as e:
            logger.error(f"âŒ å†å²æ•°æ®ä¿å­˜å¤±è´¥: {symbol} - {data_type.value} - {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
    
    async def cleanup_expired_data(self):
        """æ¸…ç†è¿‡æœŸæ•°æ®"""
        try:
            now = datetime.now()
            
            # æ¸…ç† MongoDB è¿‡æœŸç¼“å­˜
            result = self.db.cached_data.delete_many({
                "expires_at": {"$lt": now}
            })
            
            if result.deleted_count > 0:
                logger.info(f"ğŸ§¹ æ¸…ç†è¿‡æœŸç¼“å­˜æ•°æ®: {result.deleted_count} æ¡")
            
            # æ¸…ç†æ—§çš„æ–°é—»æ•°æ®ï¼ˆä¿ç•™30å¤©ï¼‰
            old_news_date = now - timedelta(days=30)
            result = self.db.news.delete_many({
                "timestamp": {"$lt": old_news_date}
            })
            
            if result.deleted_count > 0:
                logger.info(f"ğŸ§¹ æ¸…ç†æ—§æ–°é—»æ•°æ®: {result.deleted_count} æ¡")
                
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†è¿‡æœŸæ•°æ®å¤±è´¥: {e}")
    
    async def get_data_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {}

            # ç¼“å­˜æ•°æ®ç»Ÿè®¡
            for data_type in DataType:
                count = self.db.cached_data.count_documents({
                    "data_type": data_type.value
                })
                stats[f"cached_{data_type.value}"] = count

            # å†å²æ•°æ®ç»Ÿè®¡
            stats["historical_stock_info"] = self.db.stock_info.count_documents({})
            stats["historical_stock_data"] = self.db.stock_data.count_documents({})
            stats["historical_fundamentals"] = self.db.fundamentals.count_documents({})
            stats["historical_news"] = self.db.news.count_documents({})

            # Redis ç»Ÿè®¡
            redis_info = self.redis.info()
            stats["redis_used_memory"] = redis_info.get("used_memory_human", "N/A")
            stats["redis_connected_clients"] = redis_info.get("connected_clients", 0)

            # æ•°æ®æºç»Ÿè®¡
            stats["data_sources"] = self.data_source_factory.get_source_stats()

            return stats

        except Exception as e:
            logger.error(f"âŒ è·å–æ•°æ®ç»Ÿè®¡å¤±è´¥: {e}")
            return {}

    async def health_check_data_sources(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ‰€æœ‰æ•°æ®æºå¥åº·çŠ¶æ€"""
        try:
            return await self.data_source_factory.health_check_all()
        except Exception as e:
            logger.error(f"âŒ æ•°æ®æºå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return {}

    async def get_local_data_summary(self) -> Dict[str, Any]:
        """è·å–æœ¬åœ°æ•°æ®å­˜å‚¨æ‘˜è¦"""
        try:
            summary = {
                "cached_data": {},
                "historical_data": {},
                "database_info": {}
            }

            # ç¼“å­˜æ•°æ®ç»Ÿè®¡
            for data_type in DataType:
                count = self.db.cached_data.count_documents({
                    "data_type": data_type.value
                })
                summary["cached_data"][data_type.value] = count

            # å†å²æ•°æ®ç»Ÿè®¡
            summary["historical_data"]["stock_info"] = self.db.stock_info.count_documents({})
            summary["historical_data"]["stock_data"] = self.db.stock_data.count_documents({})
            summary["historical_data"]["fundamentals"] = self.db.fundamentals.count_documents({})
            summary["historical_data"]["news"] = self.db.news.count_documents({})

            # æ•°æ®åº“ä¿¡æ¯
            db_stats = self.mongodb.admin.command("dbstats")
            summary["database_info"] = {
                "database_name": "tradingagents",
                "collections": db_stats.get("collections", 0),
                "data_size": db_stats.get("dataSize", 0),
                "storage_size": db_stats.get("storageSize", 0),
                "indexes": db_stats.get("indexes", 0)
            }

            # Redis ä¿¡æ¯
            redis_info = self.redis.info()
            summary["redis_info"] = {
                "used_memory": redis_info.get("used_memory_human", "N/A"),
                "connected_clients": redis_info.get("connected_clients", 0),
                "total_commands_processed": redis_info.get("total_commands_processed", 0)
            }

            return summary

        except Exception as e:
            logger.error(f"âŒ è·å–æœ¬åœ°æ•°æ®æ‘˜è¦å¤±è´¥: {e}")
            return {}

    async def get_symbol_data_history(self, symbol: str) -> Dict[str, Any]:
        """è·å–ç‰¹å®šè‚¡ç¥¨çš„æ•°æ®å†å²"""
        try:
            history = {
                "symbol": symbol,
                "stock_info": None,
                "stock_data": [],
                "fundamentals": [],
                "news": [],
                "cached_data": []
            }

            # è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
            stock_info = self.db.stock_info.find_one({"symbol": symbol})
            if stock_info:
                history["stock_info"] = {
                    "data": stock_info.get("data"),
                    "source": stock_info.get("source"),
                    "updated_at": stock_info.get("updated_at")
                }

            # è‚¡ç¥¨ä»·æ ¼æ•°æ®ï¼ˆæœ€è¿‘30å¤©ï¼‰
            stock_data = list(self.db.stock_data.find(
                {"symbol": symbol}
            ).sort("date", -1).limit(30))

            for record in stock_data:
                history["stock_data"].append({
                    "date": record.get("date"),
                    "open": record.get("open"),
                    "high": record.get("high"),
                    "low": record.get("low"),
                    "close": record.get("close"),
                    "volume": record.get("volume"),
                    "source": record.get("source"),
                    "updated_at": record.get("updated_at")
                })

            # åŸºæœ¬é¢æ•°æ®
            fundamentals = list(self.db.fundamentals.find(
                {"symbol": symbol}
            ).sort("report_date", -1).limit(10))

            for record in fundamentals:
                history["fundamentals"].append({
                    "report_date": record.get("report_date"),
                    "data": record.get("data"),
                    "source": record.get("source"),
                    "updated_at": record.get("updated_at")
                })

            # æ–°é—»æ•°æ®ï¼ˆæœ€è¿‘20æ¡ï¼‰
            news = list(self.db.news.find(
                {"symbol": symbol}
            ).sort("publish_time", -1).limit(20))

            for record in news:
                history["news"].append({
                    "title": record.get("title"),
                    "content": record.get("content", "")[:200] + "...",  # æˆªå–å‰200å­—ç¬¦
                    "publish_time": record.get("publish_time"),
                    "source": record.get("source"),
                    "url": record.get("url")
                })

            # ç¼“å­˜æ•°æ®
            cached_data = list(self.db.cached_data.find({"symbol": symbol}))
            for record in cached_data:
                history["cached_data"].append({
                    "data_type": record.get("data_type"),
                    "source": record.get("source"),
                    "timestamp": record.get("timestamp"),
                    "expires_at": record.get("expires_at")
                })

            return history

        except Exception as e:
            logger.error(f"âŒ è·å–è‚¡ç¥¨æ•°æ®å†å²å¤±è´¥: {symbol} - {e}")
            return {}

    async def cleanup_old_data(self, days: int = 30) -> Dict[str, int]:
        """æ¸…ç†æ—§æ•°æ®"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days)
            cleanup_stats = {
                "cached_data": 0,
                "news": 0,
                "old_stock_data": 0
            }

            # æ¸…ç†è¿‡æœŸç¼“å­˜æ•°æ®
            result = self.db.cached_data.delete_many({
                "expires_at": {"$lt": cutoff_date}
            })
            cleanup_stats["cached_data"] = result.deleted_count

            # æ¸…ç†æ—§æ–°é—»æ•°æ®
            result = self.db.news.delete_many({
                "created_at": {"$lt": cutoff_date}
            })
            cleanup_stats["news"] = result.deleted_count

            # æ¸…ç†è¿‡æ—§çš„è‚¡ç¥¨æ•°æ®ï¼ˆä¿ç•™æœ€è¿‘1å¹´ï¼‰
            old_cutoff = datetime.now() - timedelta(days=365)
            result = self.db.stock_data.delete_many({
                "updated_at": {"$lt": old_cutoff}
            })
            cleanup_stats["old_stock_data"] = result.deleted_count

            logger.info(f"âœ… æ•°æ®æ¸…ç†å®Œæˆ: {cleanup_stats}")
            return cleanup_stats

        except Exception as e:
            logger.error(f"âŒ æ•°æ®æ¸…ç†å¤±è´¥: {e}")
            return {}

    async def force_refresh_data(self, symbol: str, data_type: DataType, **kwargs) -> Tuple[bool, Any]:
        """å¼ºåˆ¶åˆ·æ–°æ•°æ®ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰"""
        try:
            logger.info(f"ğŸ”„ å¼ºåˆ¶åˆ·æ–°æ•°æ®: {symbol} - {data_type.value}")

            # ç›´æ¥ä»æ•°æ®æºè·å–
            success, fresh_data = await self._fetch_from_sources(symbol, data_type, **kwargs)
            if not success:
                return False, None

            # ä¿å­˜åˆ°ç¼“å­˜å’Œæ•°æ®åº“
            await self._save_data(symbol, data_type, fresh_data)

            logger.info(f"âœ… å¼ºåˆ¶åˆ·æ–°æˆåŠŸ: {symbol} - {data_type.value}")
            return True, fresh_data

        except Exception as e:
            logger.error(f"âŒ å¼ºåˆ¶åˆ·æ–°å¤±è´¥: {symbol} - {data_type.value} - {e}")
            return False, None


# å…¨å±€æ•°æ®ç®¡ç†å™¨å®ä¾‹
data_manager: Optional[DataManager] = None

def get_data_manager() -> DataManager:
    """è·å–æ•°æ®ç®¡ç†å™¨å®ä¾‹"""
    global data_manager
    if data_manager is None:
        raise RuntimeError("æ•°æ®ç®¡ç†å™¨æœªåˆå§‹åŒ–")
    return data_manager

def init_data_manager(mongodb_client: MongoClient, redis_client: redis.Redis):
    """åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨"""
    global data_manager
    data_manager = DataManager(mongodb_client, redis_client)
    logger.info("âœ… æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
