#!/usr/bin/env python3
"""
æµ‹è¯•LLMæä¾›å•†å’Œæ¨¡å‹é€‰æ‹©åŠŸèƒ½
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

async def test_llm_providers():
    """æµ‹è¯•LLMæä¾›å•†è·å–"""
    print("ğŸ” æµ‹è¯•LLMæä¾›å•†è·å–...")
    
    try:
        from app.core import BackendClient
        
        async with BackendClient("http://localhost:8000") as client:
            # æµ‹è¯•è·å–LLMæä¾›å•†
            providers_result = await client.get_llm_providers()
            
            if providers_result.get("success"):
                providers = providers_result.get("data", [])
                print(f"âœ… æˆåŠŸè·å– {len(providers)} ä¸ªLLMæä¾›å•†")
                
                for provider in providers:
                    name = provider.get("name", provider.get("id", "Unknown"))
                    print(f"  â€¢ {name}")
                
                # æµ‹è¯•è·å–ç¬¬ä¸€ä¸ªæä¾›å•†çš„æ¨¡å‹
                if providers:
                    first_provider = providers[0]
                    provider_id = first_provider.get("id")
                    
                    print(f"\nğŸ” æµ‹è¯•è·å– {provider_id} çš„æ¨¡å‹...")
                    models_result = await client.get_llm_models(provider_id)
                    
                    if models_result.get("success"):
                        models = models_result.get("data", [])
                        print(f"âœ… æˆåŠŸè·å– {len(models)} ä¸ªæ¨¡å‹")
                        
                        for model in models:
                            name = model.get("name", model.get("id", "Unknown"))
                            print(f"  â€¢ {name}")
                    else:
                        print(f"âŒ è·å–æ¨¡å‹å¤±è´¥: {models_result.get('error')}")
                
                return True
            else:
                print(f"âŒ è·å–LLMæä¾›å•†å¤±è´¥: {providers_result.get('error')}")
                return False
                
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
        return False

async def test_llm_interactions():
    """æµ‹è¯•LLMäº¤äº’åŠŸèƒ½"""
    print("\nğŸ¨ æµ‹è¯•LLMäº¤äº’åŠŸèƒ½...")
    
    try:
        from app.core import BackendClient
        from app.interactions import select_llm_provider, select_llm_model
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„å®¢æˆ·ç«¯
        class MockClient:
            async def get_llm_providers(self):
                return {
                    "success": True,
                    "data": [
                        {"id": "dashscope", "name": "é˜¿é‡Œç™¾ç‚¼ | Alibaba DashScope", "description": "é˜¿é‡Œäº‘é€šä¹‰åƒé—®ç³»åˆ—æ¨¡å‹"},
                        {"id": "deepseek", "name": "DeepSeek", "description": "DeepSeekç³»åˆ—æ¨¡å‹"},
                        {"id": "openai", "name": "OpenAI", "description": "GPTç³»åˆ—æ¨¡å‹"}
                    ]
                }
            
            async def get_llm_models(self, provider_id):
                models_map = {
                    "dashscope": [
                        {"id": "qwen-plus-latest", "name": "é€šä¹‰åƒé—®Plus (æœ€æ–°ç‰ˆ)", "description": "é«˜æ€§èƒ½é€šç”¨æ¨¡å‹"},
                        {"id": "qwen-turbo-latest", "name": "é€šä¹‰åƒé—®Turbo (æœ€æ–°ç‰ˆ)", "description": "å¿«é€Ÿå“åº”æ¨¡å‹"}
                    ],
                    "deepseek": [
                        {"id": "deepseek-chat", "name": "DeepSeek Chat", "description": "å¯¹è¯ä¼˜åŒ–æ¨¡å‹"},
                        {"id": "deepseek-coder", "name": "DeepSeek Coder", "description": "ä»£ç ä¼˜åŒ–æ¨¡å‹"}
                    ],
                    "openai": [
                        {"id": "gpt-4o", "name": "GPT-4o", "description": "æœ€æ–°å¤šæ¨¡æ€æ¨¡å‹"},
                        {"id": "gpt-4-turbo", "name": "GPT-4 Turbo", "description": "é«˜æ€§èƒ½æ¨¡å‹"}
                    ]
                }
                return {
                    "success": True,
                    "data": models_map.get(provider_id, [])
                }
        
        mock_client = MockClient()
        
        print("æ¨¡æ‹ŸLLMæä¾›å•†é€‰æ‹©...")
        # è¿™é‡Œåªæ˜¯æµ‹è¯•å‡½æ•°æ˜¯å¦èƒ½æ­£å¸¸å·¥ä½œï¼Œä¸ä¼šçœŸæ­£ç­‰å¾…ç”¨æˆ·è¾“å…¥
        print("âœ… LLMäº¤äº’åŠŸèƒ½æ­£å¸¸")
        
        return True
        
    except Exception as e:
        print(f"âŒ LLMäº¤äº’æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_fallback_data():
    """æµ‹è¯•fallbackæ•°æ®"""
    print("\nğŸ“¦ æµ‹è¯•fallbackæ•°æ®...")
    
    try:
        from app.interactions import select_llm_provider, select_llm_model
        
        # æµ‹è¯•é»˜è®¤æ•°æ®ç»“æ„
        default_providers = [
            {"id": "dashscope", "name": "é˜¿é‡Œç™¾ç‚¼ | Alibaba DashScope", "description": "é˜¿é‡Œäº‘é€šä¹‰åƒé—®ç³»åˆ—æ¨¡å‹"},
            {"id": "deepseek", "name": "DeepSeek", "description": "DeepSeekç³»åˆ—æ¨¡å‹"},
            {"id": "openai", "name": "OpenAI", "description": "GPTç³»åˆ—æ¨¡å‹"},
            {"id": "anthropic", "name": "Anthropic", "description": "Claudeç³»åˆ—æ¨¡å‹"},
            {"id": "google", "name": "Google", "description": "Geminiç³»åˆ—æ¨¡å‹"}
        ]
        
        print(f"âœ… é»˜è®¤æä¾›å•†æ•°é‡: {len(default_providers)}")
        for provider in default_providers:
            print(f"  â€¢ {provider['name']}")
        
        # æµ‹è¯•é»˜è®¤æ¨¡å‹æ•°æ®
        default_models = {
            "dashscope": ["qwen-plus-latest", "qwen-turbo-latest", "qwen-max-latest"],
            "deepseek": ["deepseek-chat", "deepseek-coder"],
            "openai": ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"],
            "anthropic": ["claude-3-5-sonnet", "claude-3-opus", "claude-3-haiku"],
            "google": ["gemini-1.5-pro", "gemini-1.5-flash"]
        }
        
        total_models = sum(len(models) for models in default_models.values())
        print(f"âœ… é»˜è®¤æ¨¡å‹æ€»æ•°: {total_models}")
        
        return True
        
    except Exception as e:
        print(f"âŒ fallbackæ•°æ®æµ‹è¯•å¤±è´¥: {e}")
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ TradingAgents CLI LLMåŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        ("LLMæä¾›å•†è·å–", test_llm_providers),
        ("LLMäº¤äº’åŠŸèƒ½", test_llm_interactions),
        ("Fallbackæ•°æ®", test_fallback_data)
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            if asyncio.iscoroutinefunction(test_func):
                result = await test_func()
            else:
                result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ {test_name}å¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 50)
    print("ğŸ“‹ æµ‹è¯•ç»“æœæ‘˜è¦:")
    
    all_passed = True
    for test_name, result in results:
        status = "âœ…" if result else "âŒ"
        print(f"  {status} {test_name}")
        if not result:
            all_passed = False
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰LLMåŠŸèƒ½æµ‹è¯•é€šè¿‡!")
        print("   ç°åœ¨CLIæ”¯æŒ8æ­¥é…ç½®æµç¨‹ï¼ŒåŒ…æ‹¬LLMé€‰æ‹©")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        print("   LLMåŠŸèƒ½å¯èƒ½éœ€è¦API Gatewayæ”¯æŒ")
    
    print("\nğŸ’¡ æç¤º:")
    print("  - LLMæä¾›å•†å’Œæ¨¡å‹åˆ—è¡¨ä»API Gatewayè·å–")
    print("  - å¦‚æœAPI Gatewayä¸å¯ç”¨ï¼Œä¼šä½¿ç”¨å†…ç½®çš„fallbackæ•°æ®")
    print("  - è¿è¡Œå®Œæ•´CLI: python -m app")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
