"""
ç³»ç»Ÿç»´æŠ¤ç›¸å…³çš„å®šæ—¶ä»»åŠ¡
"""
import sys
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any
import asyncio
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from celery import current_task
from tasks.celery_app import celery_app

logger = logging.getLogger(__name__)


def run_async_task(coro):
    """è¿è¡Œå¼‚æ­¥ä»»åŠ¡çš„è¾…åŠ©å‡½æ•°"""
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    return loop.run_until_complete(coro)


@celery_app.task(bind=True, name='tasks.maintenance_tasks.cleanup_old_data')
def cleanup_old_data(self, days_to_keep: int = 90):
    """
    æ¸…ç†è¿‡æœŸæ•°æ®
    
    Args:
        days_to_keep: ä¿ç•™æ•°æ®çš„å¤©æ•°
    """
    task_id = self.request.id
    logger.info(f"ğŸ§¹ å¼€å§‹æ¸…ç†è¿‡æœŸæ•°æ® - ä»»åŠ¡ID: {task_id}")
    
    try:
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        
        async def cleanup_data():
            cleanup_stats = {
                'analysis_results': 0,
                'analysis_progress': 0,
                'cache_entries': 0,
                'log_files': 0
            }
            
            # æ¸…ç†è¿‡æœŸçš„åˆ†æç»“æœ
            try:
                # åˆ é™¤è¶…è¿‡ä¿ç•™æœŸçš„åˆ†æç»“æœ
                # deleted_count = await collection.delete_many({
                #     'created_at': {'$lt': cutoff_date}
                # })
                cleanup_stats['analysis_results'] = 150  # æ¨¡æ‹Ÿåˆ é™¤æ•°é‡
                logger.info(f"âœ… æ¸…ç†è¿‡æœŸåˆ†æç»“æœ: {cleanup_stats['analysis_results']}æ¡")
            except Exception as e:
                logger.error(f"âŒ æ¸…ç†åˆ†æç»“æœå¤±è´¥: {e}")
            
            # æ¸…ç†è¿‡æœŸçš„è¿›åº¦è®°å½•
            try:
                cleanup_stats['analysis_progress'] = 300  # æ¨¡æ‹Ÿåˆ é™¤æ•°é‡
                logger.info(f"âœ… æ¸…ç†è¿‡æœŸè¿›åº¦è®°å½•: {cleanup_stats['analysis_progress']}æ¡")
            except Exception as e:
                logger.error(f"âŒ æ¸…ç†è¿›åº¦è®°å½•å¤±è´¥: {e}")
            
            # æ¸…ç†Redisç¼“å­˜ä¸­çš„è¿‡æœŸæ•°æ®
            try:
                cleanup_stats['cache_entries'] = 500  # æ¨¡æ‹Ÿæ¸…ç†æ•°é‡
                logger.info(f"âœ… æ¸…ç†è¿‡æœŸç¼“å­˜: {cleanup_stats['cache_entries']}æ¡")
            except Exception as e:
                logger.error(f"âŒ æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
            
            # æ¸…ç†æ—¥å¿—æ–‡ä»¶
            try:
                cleanup_stats['log_files'] = 10  # æ¨¡æ‹Ÿæ¸…ç†æ–‡ä»¶æ•°
                logger.info(f"âœ… æ¸…ç†è¿‡æœŸæ—¥å¿—: {cleanup_stats['log_files']}ä¸ªæ–‡ä»¶")
            except Exception as e:
                logger.error(f"âŒ æ¸…ç†æ—¥å¿—å¤±è´¥: {e}")
            
            return cleanup_stats
        
        cleanup_stats = run_async_task(cleanup_data())
        
        result = {
            'cleanup_stats': cleanup_stats,
            'cutoff_date': cutoff_date.isoformat(),
            'days_kept': days_to_keep,
            'cleanup_time': datetime.now().isoformat()
        }
        
        total_cleaned = sum(cleanup_stats.values())
        logger.info(f"âœ… æ•°æ®æ¸…ç†å®Œæˆ: å…±æ¸…ç†{total_cleaned}é¡¹æ•°æ®")
        return result
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®æ¸…ç†å¤±è´¥: {e}")
        raise


@celery_app.task(bind=True, name='tasks.maintenance_tasks.refresh_cache')
def refresh_cache(self):
    """
    åˆ·æ–°ç¼“å­˜
    """
    task_id = self.request.id
    logger.info(f"ğŸ”„ å¼€å§‹åˆ·æ–°ç¼“å­˜ - ä»»åŠ¡ID: {task_id}")
    
    try:
        async def refresh_cache_data():
            refresh_stats = {
                'stock_prices': 0,
                'market_data': 0,
                'analysis_cache': 0,
                'config_cache': 0
            }
            
            # åˆ·æ–°è‚¡ç¥¨ä»·æ ¼ç¼“å­˜
            try:
                # è·å–æœ€æ–°ä»·æ ¼å¹¶æ›´æ–°ç¼“å­˜
                refresh_stats['stock_prices'] = 100  # æ¨¡æ‹Ÿåˆ·æ–°æ•°é‡
                logger.info(f"âœ… åˆ·æ–°è‚¡ç¥¨ä»·æ ¼ç¼“å­˜: {refresh_stats['stock_prices']}æ¡")
            except Exception as e:
                logger.error(f"âŒ åˆ·æ–°è‚¡ç¥¨ä»·æ ¼ç¼“å­˜å¤±è´¥: {e}")
            
            # åˆ·æ–°å¸‚åœºæ•°æ®ç¼“å­˜
            try:
                refresh_stats['market_data'] = 50
                logger.info(f"âœ… åˆ·æ–°å¸‚åœºæ•°æ®ç¼“å­˜: {refresh_stats['market_data']}æ¡")
            except Exception as e:
                logger.error(f"âŒ åˆ·æ–°å¸‚åœºæ•°æ®ç¼“å­˜å¤±è´¥: {e}")
            
            # åˆ·æ–°åˆ†æç»“æœç¼“å­˜
            try:
                refresh_stats['analysis_cache'] = 30
                logger.info(f"âœ… åˆ·æ–°åˆ†æç¼“å­˜: {refresh_stats['analysis_cache']}æ¡")
            except Exception as e:
                logger.error(f"âŒ åˆ·æ–°åˆ†æç¼“å­˜å¤±è´¥: {e}")
            
            # åˆ·æ–°é…ç½®ç¼“å­˜
            try:
                refresh_stats['config_cache'] = 10
                logger.info(f"âœ… åˆ·æ–°é…ç½®ç¼“å­˜: {refresh_stats['config_cache']}æ¡")
            except Exception as e:
                logger.error(f"âŒ åˆ·æ–°é…ç½®ç¼“å­˜å¤±è´¥: {e}")
            
            return refresh_stats
        
        refresh_stats = run_async_task(refresh_cache_data())
        
        result = {
            'refresh_stats': refresh_stats,
            'total_refreshed': sum(refresh_stats.values()),
            'refresh_time': datetime.now().isoformat()
        }
        
        logger.info(f"âœ… ç¼“å­˜åˆ·æ–°å®Œæˆ: å…±åˆ·æ–°{result['total_refreshed']}é¡¹ç¼“å­˜")
        return result
        
    except Exception as e:
        logger.error(f"âŒ ç¼“å­˜åˆ·æ–°å¤±è´¥: {e}")
        raise


@celery_app.task(bind=True, name='tasks.maintenance_tasks.archive_logs')
def archive_logs(self):
    """
    å½’æ¡£æ—¥å¿—æ–‡ä»¶
    """
    task_id = self.request.id
    logger.info(f"ğŸ“¦ å¼€å§‹å½’æ¡£æ—¥å¿— - ä»»åŠ¡ID: {task_id}")
    
    try:
        async def archive_log_files():
            archive_stats = {
                'files_archived': 0,
                'total_size_mb': 0,
                'compressed_size_mb': 0
            }
            
            # è·å–éœ€è¦å½’æ¡£çš„æ—¥å¿—æ–‡ä»¶
            log_directories = [
                '/app/logs',
                '/var/log/tradingagents'
            ]
            
            for log_dir in log_directories:
                try:
                    # æŸ¥æ‰¾è¶…è¿‡1å¤©çš„æ—¥å¿—æ–‡ä»¶
                    # å‹ç¼©å¹¶ç§»åŠ¨åˆ°å½’æ¡£ç›®å½•
                    
                    # æ¨¡æ‹Ÿå½’æ¡£è¿‡ç¨‹
                    archive_stats['files_archived'] += 5
                    archive_stats['total_size_mb'] += 100
                    archive_stats['compressed_size_mb'] += 20
                    
                    logger.info(f"âœ… å½’æ¡£ç›®å½• {log_dir} å®Œæˆ")
                    
                except Exception as e:
                    logger.error(f"âŒ å½’æ¡£ç›®å½• {log_dir} å¤±è´¥: {e}")
            
            return archive_stats
        
        archive_stats = run_async_task(archive_log_files())
        
        result = {
            'archive_stats': archive_stats,
            'compression_ratio': (
                archive_stats['compressed_size_mb'] / archive_stats['total_size_mb']
                if archive_stats['total_size_mb'] > 0 else 0
            ),
            'archive_time': datetime.now().isoformat()
        }
        
        logger.info(f"âœ… æ—¥å¿—å½’æ¡£å®Œæˆ: {archive_stats['files_archived']}ä¸ªæ–‡ä»¶")
        return result
        
    except Exception as e:
        logger.error(f"âŒ æ—¥å¿—å½’æ¡£å¤±è´¥: {e}")
        raise


@celery_app.task(bind=True, name='tasks.maintenance_tasks.backup_database')
def backup_database(self):
    """
    å¤‡ä»½æ•°æ®åº“
    """
    task_id = self.request.id
    logger.info(f"ğŸ’¾ å¼€å§‹æ•°æ®åº“å¤‡ä»½ - ä»»åŠ¡ID: {task_id}")
    
    try:
        async def backup_db():
            backup_stats = {
                'collections_backed_up': 0,
                'total_documents': 0,
                'backup_size_mb': 0,
                'backup_file': ''
            }
            
            # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_file = f"tradingagents_backup_{timestamp}.gz"
            backup_stats['backup_file'] = backup_file
            
            # å¤‡ä»½ä¸»è¦é›†åˆ
            collections_to_backup = [
                'stock_info',
                'stock_daily',
                'analysis_results',
                'system_configs'
            ]
            
            for collection in collections_to_backup:
                try:
                    # æ‰§è¡Œå¤‡ä»½å‘½ä»¤
                    # mongodump --collection=collection_name --gzip
                    
                    # æ¨¡æ‹Ÿå¤‡ä»½è¿‡ç¨‹
                    backup_stats['collections_backed_up'] += 1
                    backup_stats['total_documents'] += 10000
                    backup_stats['backup_size_mb'] += 50
                    
                    logger.info(f"âœ… å¤‡ä»½é›†åˆ {collection} å®Œæˆ")
                    
                except Exception as e:
                    logger.error(f"âŒ å¤‡ä»½é›†åˆ {collection} å¤±è´¥: {e}")
            
            # ä¸Šä¼ åˆ°å¯¹è±¡å­˜å‚¨ï¼ˆå¯é€‰ï¼‰
            try:
                # ä¸Šä¼ åˆ°MinIOæˆ–äº‘å­˜å‚¨
                logger.info(f"âœ… å¤‡ä»½æ–‡ä»¶ä¸Šä¼ å®Œæˆ: {backup_file}")
            except Exception as e:
                logger.error(f"âŒ å¤‡ä»½æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
            
            return backup_stats
        
        backup_stats = run_async_task(backup_db())
        
        result = {
            'backup_stats': backup_stats,
            'backup_time': datetime.now().isoformat(),
            'success': backup_stats['collections_backed_up'] > 0
        }
        
        logger.info(f"âœ… æ•°æ®åº“å¤‡ä»½å®Œæˆ: {backup_stats['collections_backed_up']}ä¸ªé›†åˆ")
        return result
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“å¤‡ä»½å¤±è´¥: {e}")
        raise


@celery_app.task(bind=True, name='tasks.maintenance_tasks.health_check')
def health_check(self):
    """
    ç³»ç»Ÿå¥åº·æ£€æŸ¥
    """
    task_id = self.request.id
    logger.info(f"ğŸ” å¼€å§‹ç³»ç»Ÿå¥åº·æ£€æŸ¥ - ä»»åŠ¡ID: {task_id}")
    
    try:
        async def check_system_health():
            health_status = {
                'mongodb': 'unknown',
                'redis': 'unknown',
                'api_gateway': 'unknown',
                'analysis_engine': 'unknown',
                'data_service': 'unknown',
                'disk_usage': 0,
                'memory_usage': 0,
                'cpu_usage': 0
            }
            
            # æ£€æŸ¥MongoDBè¿æ¥
            try:
                # await db_manager.client.admin.command('ping')
                health_status['mongodb'] = 'healthy'
                logger.info("âœ… MongoDB å¥åº·æ£€æŸ¥é€šè¿‡")
            except Exception as e:
                health_status['mongodb'] = 'unhealthy'
                logger.error(f"âŒ MongoDB å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            
            # æ£€æŸ¥Redisè¿æ¥
            try:
                # await redis_client.ping()
                health_status['redis'] = 'healthy'
                logger.info("âœ… Redis å¥åº·æ£€æŸ¥é€šè¿‡")
            except Exception as e:
                health_status['redis'] = 'unhealthy'
                logger.error(f"âŒ Redis å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            
            # æ£€æŸ¥å„ä¸ªå¾®æœåŠ¡
            services = ['api_gateway', 'analysis_engine', 'data_service']
            for service in services:
                try:
                    # å‘é€å¥åº·æ£€æŸ¥è¯·æ±‚
                    # response = await http_client.get(f"http://{service}:port/health")
                    health_status[service] = 'healthy'
                    logger.info(f"âœ… {service} å¥åº·æ£€æŸ¥é€šè¿‡")
                except Exception as e:
                    health_status[service] = 'unhealthy'
                    logger.error(f"âŒ {service} å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            
            # æ£€æŸ¥ç³»ç»Ÿèµ„æº
            try:
                # è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
                health_status['disk_usage'] = 45.2  # æ¨¡æ‹Ÿç£ç›˜ä½¿ç”¨ç‡
                health_status['memory_usage'] = 68.5  # æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨ç‡
                health_status['cpu_usage'] = 25.3  # æ¨¡æ‹ŸCPUä½¿ç”¨ç‡
                logger.info("âœ… ç³»ç»Ÿèµ„æºæ£€æŸ¥å®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ ç³»ç»Ÿèµ„æºæ£€æŸ¥å¤±è´¥: {e}")
            
            return health_status
        
        health_status = run_async_task(check_system_health())
        
        # è®¡ç®—æ•´ä½“å¥åº·çŠ¶æ€
        unhealthy_services = [
            service for service, status in health_status.items()
            if isinstance(status, str) and status == 'unhealthy'
        ]
        
        overall_status = 'healthy' if not unhealthy_services else 'degraded'
        if len(unhealthy_services) > 2:
            overall_status = 'unhealthy'
        
        result = {
            'overall_status': overall_status,
            'health_details': health_status,
            'unhealthy_services': unhealthy_services,
            'check_time': datetime.now().isoformat()
        }
        
        logger.info(f"âœ… ç³»ç»Ÿå¥åº·æ£€æŸ¥å®Œæˆ: {overall_status}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ ç³»ç»Ÿå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        raise
