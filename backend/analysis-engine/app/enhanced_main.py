"""
Enhanced Analysis Engine - å¢å¼ºç‰ˆåˆ†æå¼•æ“æœåŠ¡
æ”¯æŒé«˜å¹¶å‘ã€è´Ÿè½½å‡è¡¡å’Œæ™ºèƒ½ä»»åŠ¡è°ƒåº¦
"""

import asyncio
import logging
import sys
import os
from pathlib import Path
from typing import Optional, Dict, Any
from datetime import datetime
import uuid

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import uvicorn

# å¯¼å…¥å¹¶å‘ç®¡ç†æ¨¡å—
from .concurrency.concurrency_manager import get_concurrency_manager, TaskPriority, ConcurrencyManager
from .concurrency.load_balancer import get_load_balancer, LoadBalancer, LoadBalanceStrategy

# å¯¼å…¥å·¥ä½œæµç®¡ç†æ¨¡å—
from .core.workflow_scheduler import WorkflowScheduler, TaskPriority as WorkflowTaskPriority
from .core.execution_monitor import ExecutionMonitor

# å¯¼å…¥åˆ†ææ¨¡å—
from .analysis.graph_analyzer import GraphAnalyzer
from .models.requests import AnalysisRequest, AnalysisParameters
from .models.responses import APIResponse, AnalysisStatus
from .config.settings import ANALYSIS_ENGINE_CONFIG

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
graph_analyzer: Optional[GraphAnalyzer] = None
concurrency_manager: Optional[ConcurrencyManager] = None
load_balancer: Optional[LoadBalancer] = None
workflow_scheduler: Optional[WorkflowScheduler] = None
execution_monitor: Optional[ExecutionMonitor] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global graph_analyzer, concurrency_manager, load_balancer, workflow_scheduler, execution_monitor
    
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    logger.info("ğŸš€ Enhanced Analysis Engine å¯åŠ¨ä¸­...")
    
    try:
        # åˆå§‹åŒ–å¹¶å‘ç®¡ç†å™¨
        max_concurrent = ANALYSIS_ENGINE_CONFIG.get("max_concurrent_analyses", 10)
        max_queue_size = ANALYSIS_ENGINE_CONFIG.get("max_queue_size", 100)
        
        concurrency_manager = get_concurrency_manager(max_concurrent, max_queue_size)
        logger.info(f"âœ… å¹¶å‘ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ: æœ€å¤§å¹¶å‘{max_concurrent}")
        
        # åˆå§‹åŒ–è´Ÿè½½å‡è¡¡å™¨
        load_balancer = get_load_balancer(LoadBalanceStrategy.HEALTH_AWARE)
        await load_balancer.initialize()
        
        # æ·»åŠ æœ¬åœ°å®ä¾‹åˆ°è´Ÿè½½å‡è¡¡å™¨
        local_host = ANALYSIS_ENGINE_CONFIG.get("host", "localhost")
        local_port = ANALYSIS_ENGINE_CONFIG.get("port", 8005)
        load_balancer.add_instance("local", local_host, local_port, weight=1)
        
        logger.info("âœ… è´Ÿè½½å‡è¡¡å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–å›¾åˆ†æå™¨
        memory_service_url = ANALYSIS_ENGINE_CONFIG.get("memory_service_url", "http://localhost:8006")
        graph_analyzer = GraphAnalyzer(memory_service_url)
        await graph_analyzer.initialize()
        
        # è®¾ç½®ä»»åŠ¡å¤„ç†å™¨
        concurrency_manager.set_task_processor(process_analysis_task)

        logger.info("âœ… å›¾åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

        # åˆå§‹åŒ–å·¥ä½œæµè°ƒåº¦å™¨
        max_concurrent_workflows = ANALYSIS_ENGINE_CONFIG.get("max_concurrent_workflows", 3)
        workflow_scheduler = WorkflowScheduler(max_concurrent_workflows)

        # æ³¨å†Œä»»åŠ¡æ‰§è¡Œå™¨
        workflow_scheduler.register_executor("analysis", execute_analysis_workflow)
        workflow_scheduler.register_executor("debate", execute_debate_workflow)
        workflow_scheduler.register_executor("risk_assessment", execute_risk_assessment_workflow)

        await workflow_scheduler.start()
        logger.info("âœ… å·¥ä½œæµè°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆ")

        # åˆå§‹åŒ–æ‰§è¡Œç›‘æ§å™¨
        execution_monitor = ExecutionMonitor(workflow_scheduler)
        await execution_monitor.start()
        logger.info("âœ… æ‰§è¡Œç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆ")
        
        logger.info("ğŸ‰ Enhanced Analysis Engine å¯åŠ¨å®Œæˆ")
        
        yield
        
    except Exception as e:
        logger.error(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        raise
    finally:
        # å…³é—­æ—¶æ¸…ç†
        logger.info("ğŸ”„ Enhanced Analysis Engine å…³é—­ä¸­...")
        
        if concurrency_manager:
            await concurrency_manager.shutdown()
        
        if load_balancer:
            await load_balancer.cleanup()
        
        if graph_analyzer:
            await graph_analyzer.cleanup()

        if execution_monitor:
            await execution_monitor.stop()

        if workflow_scheduler:
            await workflow_scheduler.stop()

        logger.info("âœ… Enhanced Analysis Engine å·²å…³é—­")

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="Enhanced Analysis Engine",
    description="å¢å¼ºç‰ˆè‚¡ç¥¨åˆ†æå¼•æ“ - æ”¯æŒé«˜å¹¶å‘å’Œè´Ÿè½½å‡è¡¡",
    version="2.0.0",
    lifespan=lifespan
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å¯¼å…¥å¹¶æ³¨å†ŒAPIè·¯ç”±
from .api.workflow_api import router as workflow_router
app.include_router(workflow_router)

# ä¾èµ–æ³¨å…¥
async def get_concurrency_mgr() -> ConcurrencyManager:
    """è·å–å¹¶å‘ç®¡ç†å™¨"""
    if not concurrency_manager:
        raise HTTPException(status_code=503, detail="å¹¶å‘ç®¡ç†å™¨æœªåˆå§‹åŒ–")
    return concurrency_manager

async def get_load_bal() -> LoadBalancer:
    """è·å–è´Ÿè½½å‡è¡¡å™¨"""
    if not load_balancer:
        raise HTTPException(status_code=503, detail="è´Ÿè½½å‡è¡¡å™¨æœªåˆå§‹åŒ–")
    return load_balancer

# ä»»åŠ¡å¤„ç†å™¨
async def process_analysis_task(stock_code: str, analysis_type: str, 
                               metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """å¤„ç†åˆ†æä»»åŠ¡"""
    if not graph_analyzer:
        raise Exception("å›¾åˆ†æå™¨æœªåˆå§‹åŒ–")
    
    logger.info(f"ğŸ” å¼€å§‹åˆ†æ: {stock_code} ({analysis_type})")
    
    # æ‰§è¡Œåˆ†æ
    result = await graph_analyzer.analyze_stock(
        symbol=stock_code,
        analysis_type=analysis_type
    )
    
    logger.info(f"âœ… åˆ†æå®Œæˆ: {stock_code}")
    return result

# APIè·¯ç”±

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    try:
        # æ£€æŸ¥å„ç»„ä»¶çŠ¶æ€
        components = {
            "concurrency_manager": concurrency_manager is not None,
            "load_balancer": load_balancer is not None,
            "graph_analyzer": graph_analyzer is not None and graph_analyzer.initialized
        }
        
        # è·å–ç³»ç»Ÿç»Ÿè®¡
        stats = {}
        if concurrency_manager:
            stats["concurrency"] = concurrency_manager.get_statistics()
        
        if load_balancer:
            stats["load_balancer"] = load_balancer.get_instance_stats()
        
        return {
            "status": "healthy" if all(components.values()) else "unhealthy",
            "service": "enhanced-analysis-engine",
            "version": "2.0.0",
            "timestamp": datetime.now().isoformat(),
            "components": components,
            "stats": stats
        }
        
    except Exception as e:
        logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.post("/api/v1/analysis/submit", response_model=APIResponse)
async def submit_analysis(
    request: AnalysisRequest,
    priority: str = "normal",
    mgr: ConcurrencyManager = Depends(get_concurrency_mgr)
):
    """æäº¤åˆ†æä»»åŠ¡ï¼ˆé«˜å¹¶å‘ç‰ˆæœ¬ï¼‰"""
    try:
        # è§£æä¼˜å…ˆçº§
        priority_map = {
            "low": TaskPriority.LOW,
            "normal": TaskPriority.NORMAL,
            "high": TaskPriority.HIGH,
            "urgent": TaskPriority.URGENT
        }
        task_priority = priority_map.get(priority.lower(), TaskPriority.NORMAL)
        
        # æäº¤ä»»åŠ¡åˆ°å¹¶å‘ç®¡ç†å™¨
        task_id = await mgr.submit_task(
            stock_code=request.stock_code,
            analysis_type=request.analysis_type,
            priority=task_priority,
            metadata={
                "user_id": getattr(request, "user_id", None),
                "request_time": datetime.now().isoformat(),
                "parameters": request.parameters.dict() if request.parameters else {}
            }
        )
        
        logger.info(f"ğŸ“‹ åˆ†æä»»åŠ¡å·²æäº¤: {task_id} - {request.stock_code} (ä¼˜å…ˆçº§: {priority})")
        
        return APIResponse(
            success=True,
            message="åˆ†æä»»åŠ¡å·²æäº¤åˆ°é˜Ÿåˆ—",
            data={
                "task_id": task_id,
                "stock_code": request.stock_code,
                "analysis_type": request.analysis_type,
                "priority": priority,
                "estimated_wait_time": await _estimate_wait_time(mgr)
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ æäº¤åˆ†æä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æäº¤ä»»åŠ¡å¤±è´¥: {str(e)}")

@app.get("/api/v1/analysis/status/{task_id}")
async def get_task_status(
    task_id: str,
    mgr: ConcurrencyManager = Depends(get_concurrency_mgr)
):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    try:
        task = await mgr.get_task_status(task_id)
        
        if not task:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        execution_time = None
        if task.started_at and task.completed_at:
            execution_time = (task.completed_at - task.started_at).total_seconds()
        elif task.started_at:
            execution_time = (datetime.now() - task.started_at).total_seconds()
        
        return {
            "task_id": task.task_id,
            "stock_code": task.stock_code,
            "analysis_type": task.analysis_type,
            "status": task.status.value,
            "priority": task.priority.name,
            "created_at": task.created_at.isoformat(),
            "started_at": task.started_at.isoformat() if task.started_at else None,
            "completed_at": task.completed_at.isoformat() if task.completed_at else None,
            "execution_time": execution_time,
            "retry_count": task.retry_count,
            "result": task.result,
            "error": task.error
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}")

@app.delete("/api/v1/analysis/cancel/{task_id}")
async def cancel_task(
    task_id: str,
    mgr: ConcurrencyManager = Depends(get_concurrency_mgr)
):
    """å–æ¶ˆä»»åŠ¡"""
    try:
        success = await mgr.cancel_task(task_id)
        
        if success:
            return {"message": f"ä»»åŠ¡ {task_id} å·²å–æ¶ˆ"}
        else:
            raise HTTPException(status_code=400, detail="ä»»åŠ¡æ— æ³•å–æ¶ˆï¼ˆå¯èƒ½å·²åœ¨æ‰§è¡Œä¸­ï¼‰")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ å–æ¶ˆä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å–æ¶ˆä»»åŠ¡å¤±è´¥: {str(e)}")

@app.get("/api/v1/system/stats")
async def get_system_stats(
    mgr: ConcurrencyManager = Depends(get_concurrency_mgr),
    lb: LoadBalancer = Depends(get_load_bal)
):
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    try:
        return {
            "concurrency": mgr.get_statistics(),
            "queue": mgr.get_queue_status(),
            "load_balancer": lb.get_instance_stats(),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ è·å–ç³»ç»Ÿç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡å¤±è´¥: {str(e)}")

@app.post("/api/v1/admin/cleanup")
async def cleanup_completed_tasks(
    max_age_hours: int = 24,
    mgr: ConcurrencyManager = Depends(get_concurrency_mgr)
):
    """æ¸…ç†å®Œæˆçš„ä»»åŠ¡"""
    try:
        await mgr.cleanup_completed_tasks(max_age_hours)
        return {"message": f"å·²æ¸…ç†è¶…è¿‡{max_age_hours}å°æ—¶çš„å®Œæˆä»»åŠ¡"}
        
    except Exception as e:
        logger.error(f"âŒ æ¸…ç†ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…ç†å¤±è´¥: {str(e)}")

@app.post("/api/v1/admin/load_balancer/add_instance")
async def add_instance(
    instance_id: str,
    host: str,
    port: int,
    weight: int = 1,
    lb: LoadBalancer = Depends(get_load_bal)
):
    """æ·»åŠ Analysis Engineå®ä¾‹"""
    try:
        lb.add_instance(instance_id, host, port, weight)
        return {"message": f"å®ä¾‹ {instance_id} å·²æ·»åŠ "}
        
    except Exception as e:
        logger.error(f"âŒ æ·»åŠ å®ä¾‹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ·»åŠ å®ä¾‹å¤±è´¥: {str(e)}")

@app.delete("/api/v1/admin/load_balancer/remove_instance/{instance_id}")
async def remove_instance(
    instance_id: str,
    lb: LoadBalancer = Depends(get_load_bal)
):
    """ç§»é™¤Analysis Engineå®ä¾‹"""
    try:
        lb.remove_instance(instance_id)
        return {"message": f"å®ä¾‹ {instance_id} å·²ç§»é™¤"}
        
    except Exception as e:
        logger.error(f"âŒ ç§»é™¤å®ä¾‹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç§»é™¤å®ä¾‹å¤±è´¥: {str(e)}")

# è¾…åŠ©å‡½æ•°
async def _estimate_wait_time(mgr: ConcurrencyManager) -> float:
    """ä¼°ç®—ç­‰å¾…æ—¶é—´"""
    stats = mgr.get_statistics()
    queue_size = stats["current_queued"]
    avg_execution_time = stats["average_execution_time"]
    concurrent_tasks = stats["current_running"]
    max_concurrent = mgr.max_concurrent_tasks
    
    if queue_size == 0:
        return 0.0
    
    # ç®€å•ä¼°ç®—ï¼šé˜Ÿåˆ—ä»»åŠ¡æ•° / å¯ç”¨å¹¶å‘æ•° * å¹³å‡æ‰§è¡Œæ—¶é—´
    available_slots = max(1, max_concurrent - concurrent_tasks)
    estimated_time = (queue_size / available_slots) * max(avg_execution_time, 30)
    
    return estimated_time

# å·¥ä½œæµæ‰§è¡Œå™¨å‡½æ•°
async def execute_analysis_workflow(task):
    """æ‰§è¡Œåˆ†æå·¥ä½œæµ"""
    try:
        logger.info(f"ğŸ” æ‰§è¡Œåˆ†æå·¥ä½œæµ: {task.symbol}")

        # ä½¿ç”¨å›¾åˆ†æå™¨æ‰§è¡Œå®Œæ•´åˆ†æ
        if graph_analyzer:
            result = await graph_analyzer.analyze_stock(
                symbol=task.symbol,
                analysis_date=datetime.now().strftime("%Y-%m-%d"),
                **task.metadata
            )
            return result
        else:
            raise Exception("å›¾åˆ†æå™¨æœªåˆå§‹åŒ–")

    except Exception as e:
        logger.error(f"âŒ åˆ†æå·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
        raise

async def execute_debate_workflow(task):
    """æ‰§è¡Œè¾©è®ºå·¥ä½œæµ"""
    try:
        logger.info(f"ğŸ­ æ‰§è¡Œè¾©è®ºå·¥ä½œæµ: {task.symbol}")

        # è¿™é‡Œå¯ä»¥å®ç°ä¸“é—¨çš„è¾©è®ºæµç¨‹
        # ç›®å‰ä½¿ç”¨å›¾åˆ†æå™¨çš„è¾©è®ºåŠŸèƒ½
        if graph_analyzer:
            result = await graph_analyzer.analyze_stock(
                symbol=task.symbol,
                analysis_date=datetime.now().strftime("%Y-%m-%d"),
                focus="debate",
                **task.metadata
            )
            return result
        else:
            raise Exception("å›¾åˆ†æå™¨æœªåˆå§‹åŒ–")

    except Exception as e:
        logger.error(f"âŒ è¾©è®ºå·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
        raise

async def execute_risk_assessment_workflow(task):
    """æ‰§è¡Œé£é™©è¯„ä¼°å·¥ä½œæµ"""
    try:
        logger.info(f"âš ï¸ æ‰§è¡Œé£é™©è¯„ä¼°å·¥ä½œæµ: {task.symbol}")

        # è¿™é‡Œå¯ä»¥å®ç°ä¸“é—¨çš„é£é™©è¯„ä¼°æµç¨‹
        if graph_analyzer:
            result = await graph_analyzer.analyze_stock(
                symbol=task.symbol,
                analysis_date=datetime.now().strftime("%Y-%m-%d"),
                focus="risk_assessment",
                **task.metadata
            )
            return result
        else:
            raise Exception("å›¾åˆ†æå™¨æœªåˆå§‹åŒ–")

    except Exception as e:
        logger.error(f"âŒ é£é™©è¯„ä¼°å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    # è·å–é…ç½®
    host = ANALYSIS_ENGINE_CONFIG.get("host", "0.0.0.0")
    port = ANALYSIS_ENGINE_CONFIG.get("port", 8005)
    debug = ANALYSIS_ENGINE_CONFIG.get("debug", False)
    
    logger.info(f"ğŸš€ å¯åŠ¨Enhanced Analysis Engine: {host}:{port}")
    
    uvicorn.run(
        "enhanced_main:app",
        host=host,
        port=port,
        reload=debug,
        log_level="info"
    )
