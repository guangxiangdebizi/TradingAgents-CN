#!/usr/bin/env python3
"""
LLM Service - å¤§æ¨¡å‹ç»Ÿä¸€è°ƒç”¨æœåŠ¡
æä¾›æ ‡å‡†åŒ–çš„å¤§æ¨¡å‹APIæ¥å£
"""

import asyncio
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
import json

from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import redis.asyncio as redis

# å¯¼å…¥é€‚é…å™¨å’Œè·¯ç”±å™¨
from .adapters.factory import get_adapter_factory
from .routing.model_router import ModelRouter
from .tracking.usage_tracker import UsageTracker
from .prompts.prompt_manager import get_prompt_manager as create_prompt_manager
from .models.requests import ChatCompletionRequest, ModelListRequest, ChatMessage
from .models.responses import ChatCompletionResponse, ModelListResponse, UsageStatsResponse
from .config.settings import LLM_SERVICE_CONFIG

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="LLM Service",
    description="å¤§æ¨¡å‹ç»Ÿä¸€è°ƒç”¨æœåŠ¡",
    version="1.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡
redis_client: Optional[redis.Redis] = None
adapter_factory = None
model_router = None
usage_tracker = None
prompt_manager = None

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨äº‹ä»¶"""
    global redis_client, adapter_factory, model_router, usage_tracker, prompt_manager

    logger.info("ğŸš€ å¯åŠ¨LLM Service...")

    # åˆå§‹åŒ–Redisè¿æ¥
    try:
        redis_client = redis.Redis(
            host=LLM_SERVICE_CONFIG.get("redis_host", "localhost"),
            port=LLM_SERVICE_CONFIG.get("redis_port", 6379),
            decode_responses=True
        )
        await redis_client.ping()
        logger.info("âœ… Redisè¿æ¥æˆåŠŸ")
    except Exception as e:
        logger.warning(f"âš ï¸ Redisè¿æ¥å¤±è´¥: {e}")
        redis_client = None

    # åˆå§‹åŒ–æç¤ºè¯ç®¡ç†å™¨
    global prompt_manager
    prompt_manager = await create_prompt_manager()
    logger.info("âœ… æç¤ºè¯ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    # åˆå§‹åŒ–é€‚é…å™¨å·¥å‚
    adapter_factory = get_adapter_factory()
    logger.info("âœ… é€‚é…å™¨å·¥å‚åˆå§‹åŒ–å®Œæˆ")

    # åˆå§‹åŒ–æ¨¡å‹è·¯ç”±å™¨
    adapters = await adapter_factory.get_all_adapters()
    model_router = ModelRouter(adapters)
    logger.info(f"âœ… æ¨¡å‹è·¯ç”±å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ”¯æŒ{len(adapters)}ä¸ªæ¨¡å‹")

    # åˆå§‹åŒ–ä½¿ç”¨ç»Ÿè®¡å™¨
    usage_tracker = UsageTracker(redis_client=redis_client)
    logger.info("âœ… ä½¿ç”¨ç»Ÿè®¡å™¨åˆå§‹åŒ–å®Œæˆ")

    logger.info("ğŸ‰ LLM Serviceå¯åŠ¨å®Œæˆ")

@app.on_event("shutdown")
async def shutdown_event():
    """å…³é—­äº‹ä»¶"""
    global redis_client
    
    logger.info("ğŸ›‘ å…³é—­LLM Service...")
    
    if redis_client:
        await redis_client.close()
        logger.info("âœ… Redisè¿æ¥å·²å…³é—­")
    
    logger.info("ğŸ‘‹ LLM Serviceå·²å…³é—­")

# ä¾èµ–æ³¨å…¥
async def get_redis() -> Optional[redis.Redis]:
    """è·å–Rediså®¢æˆ·ç«¯"""
    return redis_client

async def get_model_router() -> ModelRouter:
    """è·å–æ¨¡å‹è·¯ç”±å™¨"""
    if not model_router:
        raise HTTPException(status_code=503, detail="æ¨¡å‹è·¯ç”±å™¨æœªåˆå§‹åŒ–")
    return model_router

async def get_usage_tracker() -> UsageTracker:
    """è·å–ä½¿ç”¨ç»Ÿè®¡å™¨"""
    if not usage_tracker:
        raise HTTPException(status_code=503, detail="ä½¿ç”¨ç»Ÿè®¡å™¨æœªåˆå§‹åŒ–")
    return usage_tracker

async def get_prompt_manager():
    """è·å–æç¤ºè¯ç®¡ç†å™¨"""
    if not prompt_manager:
        raise HTTPException(status_code=503, detail="æç¤ºè¯ç®¡ç†å™¨æœªåˆå§‹åŒ–")
    return prompt_manager

# ===== APIæ¥å£ =====

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    dependencies = {}
    
    # æ£€æŸ¥Redis
    if redis_client:
        try:
            await redis_client.ping()
            dependencies["redis"] = "connected"
        except Exception:
            dependencies["redis"] = "disconnected"
    else:
        dependencies["redis"] = "not_configured"
    
    # æ£€æŸ¥æ¨¡å‹é€‚é…å™¨ï¼ˆåªæ£€æŸ¥æ˜¯å¦åˆå§‹åŒ–ï¼Œä¸æ£€æŸ¥è¿é€šæ€§ï¼‰
    if model_router:
        available_models = list(model_router.adapters.keys())
        dependencies["models"] = f"{len(available_models)} initialized"
        dependencies["model_list"] = available_models
    else:
        dependencies["models"] = "not_initialized"
        dependencies["model_list"] = []
    
    return {
        "status": "healthy",
        "service": "llm-service",
        "timestamp": datetime.now().isoformat(),
        "dependencies": dependencies
    }

@app.post("/api/v1/chat/completions", response_model=ChatCompletionResponse)
async def chat_completions(
    request: ChatCompletionRequest,
    background_tasks: BackgroundTasks,
    router: ModelRouter = Depends(get_model_router),
    tracker: UsageTracker = Depends(get_usage_tracker),
    prompt_mgr = Depends(get_prompt_manager)
):
    """èŠå¤©å®Œæˆæ¥å£"""
    try:
        start_time = datetime.now()

        # è®°å½•è¯·æ±‚è¯¦æƒ…
        logger.info(f"ğŸ” LLMè¯·æ±‚è¯¦æƒ…: model={request.model}, task_type={request.task_type}, messages_count={len(request.messages)}")
        logger.info(f"ğŸ” LLMè¯·æ±‚æ¶ˆæ¯: {[{'role': msg.role, 'content': msg.content[:200] + '...' if len(msg.content) > 200 else msg.content} for msg in request.messages]}")

        # 1. è·¯ç”±åˆ°æœ€é€‚åˆçš„æ¨¡å‹
        selected_model = await router.route_request(
            task_type=request.task_type,
            model_preference=request.model
        )

        logger.info(f"ğŸ¯ è·¯ç”±åˆ°æ¨¡å‹: {selected_model} (ä»»åŠ¡ç±»å‹: {request.task_type})")
        
        # 2. è·å–é€‚é…å™¨
        adapter = router.adapters[selected_model]

        # 3. å¤„ç†æç¤ºè¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
        messages_to_send = request.messages

        # å¦‚æœæ¶ˆæ¯åªæœ‰ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼Œå°è¯•ä½¿ç”¨æç¤ºè¯æ¨¡æ¿
        if (len(request.messages) == 1 and
            request.messages[0].role == "user" and
            hasattr(request, 'use_prompt_template') and
            getattr(request, 'use_prompt_template', True)):

            try:
                # æå–ç”¨æˆ·è¾“å…¥
                user_input = request.messages[0].content

                # ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨æ ¼å¼åŒ–æ¶ˆæ¯
                formatted_messages = prompt_mgr.format_messages(
                    model=selected_model,
                    task_type=request.task_type,
                    variables={"user_input": user_input},
                    language="zh"
                )

                if formatted_messages:
                    messages_to_send = formatted_messages
                    logger.info(f"ğŸ¯ ä½¿ç”¨æç¤ºè¯æ¨¡æ¿: {selected_model} - {request.task_type}")

            except Exception as e:
                logger.warning(f"âš ï¸ æç¤ºè¯å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯: {e}")

        # 4. è½¬æ¢æ¶ˆæ¯æ ¼å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if isinstance(messages_to_send[0], ChatMessage):
            # è½¬æ¢ChatMessageå¯¹è±¡ä¸ºå­—å…¸
            messages_dict = [
                {"role": msg.role, "content": msg.content}
                for msg in messages_to_send
            ]
        else:
            # å·²ç»æ˜¯å­—å…¸æ ¼å¼
            messages_dict = messages_to_send

        # 5. è°ƒç”¨æ¨¡å‹
        logger.info(f"ğŸ” è°ƒç”¨æ¨¡å‹å‰: {selected_model}, messages_count={len(messages_dict)}")
        logger.info(f"ğŸ” å‘é€ç»™æ¨¡å‹çš„æ¶ˆæ¯: {[{'role': msg['role'], 'content': msg['content'][:200] + '...' if len(msg['content']) > 200 else msg['content']} for msg in messages_dict]}")

        result = await adapter.chat_completion(
            messages=messages_dict,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            stream=request.stream
        )

        logger.info(f"ğŸ” æ¨¡å‹è°ƒç”¨ç»“æœ: success={result.get('success')}, content_length={len(result.get('content', ''))}")
        if result.get("success"):
            logger.info(f"ğŸ” æ¨¡å‹è¿”å›å†…å®¹: {result.get('content', '')[:300]}...")
        else:
            logger.error(f"ğŸ” æ¨¡å‹è°ƒç”¨å¤±è´¥è¯¦æƒ…: {result}")

        if not result.get("success"):
            raise HTTPException(status_code=500, detail=f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {result.get('error')}")
        
        # 4. æ„å»ºå“åº”
        response = ChatCompletionResponse(
            id=f"chatcmpl-{int(datetime.now().timestamp())}",
            object="chat.completion",
            created=int(start_time.timestamp()),
            model=selected_model,
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": result["content"]
                },
                "finish_reason": "stop"
            }],
            usage={
                "prompt_tokens": result.get("usage", {}).get("prompt_tokens", 0),
                "completion_tokens": result.get("usage", {}).get("completion_tokens", 0),
                "total_tokens": result.get("usage", {}).get("total_tokens", 0)
            }
        )

        logger.info(f"ğŸ” æ„å»ºå“åº”å®Œæˆ: model={selected_model}, content_length={len(result['content'])}, tokens={response.usage.total_tokens}")
        logger.info(f"ğŸ” æœ€ç»ˆå“åº”å†…å®¹: {result['content'][:300]}...")
        
        # 5. åå°è®°å½•ä½¿ç”¨ç»Ÿè®¡
        background_tasks.add_task(
            tracker.track_usage,
            user_id=request.user_id or "anonymous",
            model=selected_model,
            task_type=request.task_type,
            usage={
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            },
            duration=(datetime.now() - start_time).total_seconds()
        )
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ èŠå¤©å®Œæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {str(e)}")

@app.get("/api/v1/models", response_model=ModelListResponse)
async def list_models(
    router: ModelRouter = Depends(get_model_router)
):
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    try:
        models = []
        
        for model_name, adapter in router.adapters.items():
            try:
                model_info = adapter.get_model_info()
                # åªæ£€æŸ¥é€‚é…å™¨æ˜¯å¦å¯ç”¨ï¼Œä¸åšå¥åº·æ£€æŸ¥
                is_enabled = adapter.is_enabled()

                models.append({
                    "id": model_name,
                    "object": "model",
                    "provider": model_info.get("provider", "unknown"),
                    "max_tokens": model_info.get("max_tokens", 4096),
                    "supports_streaming": model_info.get("supports_streaming", False),
                    "cost_per_1k_tokens": model_info.get("cost_per_1k_tokens", {}),
                    "strengths": model_info.get("strengths", []),
                    "best_for": model_info.get("best_for", []),
                    "status": "available" if is_enabled else "disabled"
                })
            except Exception as e:
                logger.warning(f"âš ï¸ è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥ {model_name}: {e}")
        
        return ModelListResponse(
            object="list",
            data=models
        )
        
    except Exception as e:
        logger.error(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/v1/usage/stats", response_model=UsageStatsResponse)
async def get_usage_stats(
    user_id: Optional[str] = None,
    model: Optional[str] = None,
    days: int = 7,
    tracker: UsageTracker = Depends(get_usage_tracker)
):
    """è·å–ä½¿ç”¨ç»Ÿè®¡"""
    try:
        stats = await tracker.get_usage_stats(
            user_id=user_id,
            model=model,
            days=days
        )
        
        return UsageStatsResponse(
            success=True,
            data=stats
        )
        
    except Exception as e:
        logger.error(f"âŒ è·å–ä½¿ç”¨ç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä½¿ç”¨ç»Ÿè®¡å¤±è´¥: {str(e)}")

@app.get("/api/v1/prompts/templates")
async def list_prompt_templates(
    model_type: Optional[str] = None,
    task_type: Optional[str] = None,
    language: Optional[str] = None,
    prompt_mgr = Depends(get_prompt_manager)
):
    """è·å–æç¤ºè¯æ¨¡æ¿åˆ—è¡¨"""
    try:
        templates = prompt_mgr.list_templates(
            model_type=model_type,
            task_type=task_type,
            language=language
        )

        return {
            "success": True,
            "data": [template.to_dict() for template in templates],
            "total": len(templates)
        }

    except Exception as e:
        logger.error(f"âŒ è·å–æç¤ºè¯æ¨¡æ¿å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æç¤ºè¯æ¨¡æ¿å¤±è´¥: {str(e)}")

@app.get("/api/v1/prompts/stats")
async def get_prompt_stats(prompt_mgr = Depends(get_prompt_manager)):
    """è·å–æç¤ºè¯ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = prompt_mgr.get_stats()
        return {
            "success": True,
            "data": stats
        }
    except Exception as e:
        logger.error(f"âŒ è·å–æç¤ºè¯ç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æç¤ºè¯ç»Ÿè®¡å¤±è´¥: {str(e)}")

@app.post("/api/v1/admin/reload-models")
async def reload_models():
    """é‡æ–°åŠ è½½æ¨¡å‹é€‚é…å™¨"""
    global model_router

    try:
        # é‡æ–°åˆå§‹åŒ–é€‚é…å™¨
        adapters = await adapter_factory.get_all_adapters()
        model_router = ModelRouter(adapters)

        logger.info(f"âœ… æ¨¡å‹é‡æ–°åŠ è½½å®Œæˆï¼Œæ”¯æŒ{len(adapters)}ä¸ªæ¨¡å‹")

        return {
            "success": True,
            "message": f"æˆåŠŸé‡æ–°åŠ è½½{len(adapters)}ä¸ªæ¨¡å‹",
            "models": list(adapters.keys())
        }

    except Exception as e:
        logger.error(f"âŒ é‡æ–°åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"é‡æ–°åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")

@app.post("/api/v1/admin/reload-prompts")
async def reload_prompts(prompt_mgr = Depends(get_prompt_manager)):
    """é‡æ–°åŠ è½½æç¤ºè¯æ¨¡æ¿"""
    try:
        await prompt_mgr.reload_templates()
        stats = prompt_mgr.get_stats()

        logger.info(f"âœ… æç¤ºè¯é‡æ–°åŠ è½½å®Œæˆï¼Œå…±{stats['total_templates']}ä¸ªæ¨¡æ¿")

        return {
            "success": True,
            "message": f"æˆåŠŸé‡æ–°åŠ è½½{stats['total_templates']}ä¸ªæç¤ºè¯æ¨¡æ¿",
            "stats": stats
        }

    except Exception as e:
        logger.error(f"âŒ é‡æ–°åŠ è½½æç¤ºè¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"é‡æ–°åŠ è½½æç¤ºè¯å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    import sys
    from pathlib import Path

    # æ·»åŠ sharedè·¯å¾„
    shared_path = Path(__file__).parent.parent.parent / "shared"
    sys.path.insert(0, str(shared_path))

    from utils.config import get_config

    config = get_config()
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=config.get('LLM_SERVICE_PORT', 8004),
        reload=config.get('DEBUG', False),
        log_level=config.get('LOG_LEVEL', 'INFO').lower()
    )
