# Backendæ ¸å¿ƒæœåŠ¡å…³ç³»è¯´æ˜æ–‡æ¡£

## ğŸ“‹ **æ¦‚è¿°**

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜Backendé¡¹ç›®ä¸­ä¸‰å¤§æ ¸å¿ƒæœåŠ¡ï¼ˆAnalysis Engineã€Agent Serviceã€LLM Serviceï¼‰ä¹‹é—´çš„å…³ç³»ã€è°ƒç”¨æµç¨‹å’Œæ•…éšœæ’æŸ¥æ–¹æ³•ã€‚

## ğŸ—ï¸ **æœåŠ¡æ¶æ„æ¦‚è§ˆ**

### **æœåŠ¡å±‚æ¬¡ç»“æ„**

```
ç”¨æˆ·è¯·æ±‚å±‚
    â†“
ä¸šåŠ¡é€»è¾‘å±‚ (Analysis Engine :8002)
    â†“
æ™ºèƒ½ä½“æœåŠ¡å±‚ (Agent Service :8005)
    â†“
æ¨¡å‹è°ƒç”¨å±‚ (LLM Service :8004)
```

### **ç«¯å£åˆ†é…**
- **Analysis Engine**: 8002 (åˆ†æå¼•æ“ + å›¾å¼•æ“)
- **Agent Service**: 8005 (æ™ºèƒ½ä½“ç®¡ç† + è¾©è®ºå¼•æ“)
- **LLM Service**: 8004 (å¤§æ¨¡å‹ç»Ÿä¸€è°ƒç”¨)
- **Data Service**: 8003 (æ•°æ®æœåŠ¡)
- **API Gateway**: 8001 (ç»Ÿä¸€å…¥å£)
- **Memory Service**: 8006 (å†…å­˜å’ŒçŠ¶æ€ç®¡ç†)

## ğŸ”„ **æœåŠ¡é—´è°ƒç”¨å…³ç³»**

### **1. Analysis Engine â†’ Agent Service**

#### **è°ƒç”¨æ–¹å¼**
```python
# ä½ç½®: backend/analysis-engine/app/graphs/agent_nodes.py
class AgentNodes:
    def __init__(self):
        self.agent_service_url = "http://agent-service:8005"
    
    async def _call_agent_service(self, agent_type: str, action: str, data: Dict[str, Any]):
        url = f"{self.agent_service_url}/api/v1/agents/{agent_type}/{action}"
        async with self.session.post(url, json=data) as response:
            return await response.json()
```

#### **è°ƒç”¨åœºæ™¯**
- **å›¾èŠ‚ç‚¹æ‰§è¡Œ**: æ¯ä¸ªå›¾èŠ‚ç‚¹è°ƒç”¨å¯¹åº”çš„Agent
- **å¤šè½®è¾©è®º**: Bull/Bearç ”ç©¶å‘˜çš„è¾©è®ºäº¤äº’
- **é£é™©åˆ†æ**: ä¸‰æ–¹é£é™©åˆ†æå¸ˆçš„è½®æµåˆ†æ

#### **APIæ¥å£**
```
POST /api/v1/agents/{agent_type}/{action}

æ”¯æŒçš„agent_type:
- market_analyst (å¸‚åœºåˆ†æå¸ˆ)
- fundamentals_analyst (åŸºæœ¬é¢åˆ†æå¸ˆ)
- news_analyst (æ–°é—»åˆ†æå¸ˆ)
- bull_researcher (å¤šå¤´ç ”ç©¶å‘˜)
- bear_researcher (ç©ºå¤´ç ”ç©¶å‘˜)
- research_manager (ç ”ç©¶ç»ç†)
- risky_analyst (æ¿€è¿›åˆ†æå¸ˆ)
- safe_analyst (ä¿å®ˆåˆ†æå¸ˆ)
- neutral_analyst (ä¸­æ€§åˆ†æå¸ˆ)
- risk_manager (é£é™©ç»ç†)
- trader (äº¤æ˜“å‘˜)

æ”¯æŒçš„action:
- analyze (åˆ†æ)
- research (ç ”ç©¶)
- assess (è¯„ä¼°)
- plan (è®¡åˆ’)
- synthesize (ç»¼åˆ)
```

### **2. Agent Service â†’ LLM Service**

#### **è°ƒç”¨æ–¹å¼**
```python
# ä½ç½®: backend/agent-service/app/agents/analysts/market_analyst.py
async def _generate_analysis_report(self, context, market_data, ...):
    response = await self.llm_client.chat_completion(
        messages=[{"role": "user", "content": prompt}],
        model="deepseek-chat",
        temperature=0.1
    )
```

#### **è°ƒç”¨åœºæ™¯**
- **æ™ºèƒ½ä½“æ€è€ƒ**: æ¯ä¸ªAgentéœ€è¦LLMè¿›è¡Œæ¨ç†
- **æŠ¥å‘Šç”Ÿæˆ**: ç”Ÿæˆå„ç§åˆ†ææŠ¥å‘Š
- **è¾©è®ºå¯¹è¯**: å¤šè½®è¾©è®ºä¸­çš„è§‚ç‚¹ç”Ÿæˆ

#### **APIæ¥å£**
```
POST /api/v1/chat/completions

è¯·æ±‚æ ¼å¼:
{
    "model": "deepseek-chat",
    "messages": [
        {"role": "user", "content": "åˆ†æå†…å®¹"}
    ],
    "temperature": 0.1,
    "max_tokens": 1500
}

å“åº”æ ¼å¼:
{
    "success": true,
    "content": "LLMç”Ÿæˆçš„å†…å®¹",
    "usage": {
        "prompt_tokens": 100,
        "completion_tokens": 200,
        "total_tokens": 300
    }
}
```

### **3. Analysis Engine â†’ Data Service**

#### **è°ƒç”¨æ–¹å¼**
```python
# ä½ç½®: backend/analysis-engine/app/tools/data_tools.py
async def get_stock_data(symbol: str):
    async with aiohttp.ClientSession() as session:
        url = f"http://data-service:8003/api/v1/stocks/{symbol}/data"
        async with session.get(url) as response:
            return await response.json()
```

#### **è°ƒç”¨åœºæ™¯**
- **æ•°æ®è·å–**: è·å–è‚¡ç¥¨åŸºç¡€æ•°æ®ã€è´¢åŠ¡æ•°æ®ã€æ–°é—»æ•°æ®
- **å·¥å…·è°ƒç”¨**: å›¾å¼•æ“ä¸­çš„å·¥å…·èŠ‚ç‚¹è°ƒç”¨

## ğŸ“Š **å®Œæ•´è°ƒç”¨æµç¨‹**

### **è‚¡ç¥¨åˆ†æå®Œæ•´æµç¨‹**

```
1. ç”¨æˆ·è¯·æ±‚
   POST /api/v1/analysis/comprehensive
   â†’ API Gateway (8001)

2. è·¯ç”±åˆ°åˆ†æå¼•æ“
   â†’ Analysis Engine (8002)
   â†’ TradingGraph.analyze_stock()

3. å›¾å¼•æ“æ‰§è¡Œ
   â†’ GraphState åˆå§‹åŒ–
   â†’ æ¡ä»¶é€»è¾‘åˆ¤æ–­
   â†’ èŠ‚ç‚¹é¡ºåºæ‰§è¡Œ

4. åˆ†æå¸ˆèŠ‚ç‚¹æ‰§è¡Œ
   MarketAnalyst Node:
   â†’ è°ƒç”¨ Agent Service (8005)
   â†’ POST /api/v1/agents/market_analyst/analyze
   
   Agent Service:
   â†’ AgentManager.execute_task()
   â†’ MarketAnalyst.analyze()
   â†’ è°ƒç”¨ LLM Service (8004)
   â†’ POST /api/v1/chat/completions
   
   LLM Service:
   â†’ ModelRouter è·¯ç”±
   â†’ DeepSeekAdapter æ‰§è¡Œ
   â†’ è¿”å›åˆ†æç»“æœ

5. å¤šè½®è¾©è®ºæ‰§è¡Œ
   Bull Researcher Node:
   â†’ è°ƒç”¨ Agent Service (8005)
   â†’ BullResearcher.research()
   â†’ è°ƒç”¨ LLM Service (8004)
   
   Bear Researcher Node:
   â†’ è°ƒç”¨ Agent Service (8005)
   â†’ BearResearcher.research()
   â†’ è°ƒç”¨ LLM Service (8004)
   
   æ¡ä»¶é€»è¾‘åˆ¤æ–­:
   â†’ ConditionalLogic.should_continue_debate()
   â†’ å†³å®šç»§ç»­è¾©è®ºæˆ–ç»“æŸ

6. é£é™©åˆ†ææ‰§è¡Œ
   (ç±»ä¼¼è¾©è®ºæµç¨‹ï¼Œä¸‰æ–¹è½®æµåˆ†æ)

7. æœ€ç»ˆç»“æœ
   â†’ å›¾å¼•æ“æ±‡æ€»æ‰€æœ‰ç»“æœ
   â†’ è¿”å›å®Œæ•´åˆ†ææŠ¥å‘Š
```

## ğŸ”§ **é…ç½®å’Œè¿æ¥**

### **æœåŠ¡å‘ç°é…ç½®**

#### **Analysis Engineé…ç½®**
```python
# backend/analysis-engine/app/graphs/agent_nodes.py
class AgentNodes:
    def __init__(self):
        self.agent_service_url = "http://agent-service:8005"
        # Dockerç¯å¢ƒä½¿ç”¨æœåŠ¡åï¼Œæœ¬åœ°å¼€å‘ä½¿ç”¨localhost
```

#### **Agent Serviceé…ç½®**
```python
# backend/agent-service/app/agents/base_agent.py
class BaseAgent:
    def __init__(self):
        self.llm_service_url = "http://llm-service:8004"
        # é€šè¿‡ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è®¾ç½®
```

#### **ç¯å¢ƒå˜é‡é…ç½®**
```bash
# Docker Composeç¯å¢ƒ
AGENT_SERVICE_URL=http://agent-service:8005
LLM_SERVICE_URL=http://llm-service:8004
DATA_SERVICE_URL=http://data-service:8003

# æœ¬åœ°å¼€å‘ç¯å¢ƒ
AGENT_SERVICE_URL=http://localhost:8005
LLM_SERVICE_URL=http://localhost:8004
DATA_SERVICE_URL=http://localhost:8003
```

### **è¿æ¥æ± å’Œè¶…æ—¶é…ç½®**

#### **HTTPå®¢æˆ·ç«¯é…ç½®**
```python
# è¿æ¥è¶…æ—¶é…ç½®
timeout_config = {
    "total": 300,      # æ€»è¶…æ—¶æ—¶é—´
    "connect": 30,     # è¿æ¥è¶…æ—¶
    "read": 270        # è¯»å–è¶…æ—¶
}

# é‡è¯•é…ç½®
retry_config = {
    "max_retries": 3,
    "retry_delay": 5,
    "backoff_factor": 2
}
```

## ğŸš¨ **æ•…éšœæ’æŸ¥æŒ‡å—**

### **å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ**

#### **1. æœåŠ¡è¿æ¥å¤±è´¥**

**ç—‡çŠ¶**: 
```
âŒ AgentæœåŠ¡è°ƒç”¨å¤±è´¥: market_analyst/analyze - Connection refused
```

**æ’æŸ¥æ­¥éª¤**:
```bash
# 1. æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker ps | grep agent-service
docker logs backend-agent-service-1

# 2. æ£€æŸ¥ç½‘ç»œè¿æ¥
docker exec -it backend-analysis-engine-1 ping agent-service
curl http://agent-service:8005/health

# 3. æ£€æŸ¥ç«¯å£æ˜ å°„
docker port backend-agent-service-1
```

**è§£å†³æ–¹æ¡ˆ**:
- ç¡®ä¿Agent Serviceæ­£å¸¸å¯åŠ¨
- æ£€æŸ¥Dockerç½‘ç»œé…ç½®
- éªŒè¯æœåŠ¡å‘ç°é…ç½®

#### **2. LLMè°ƒç”¨è¶…æ—¶**

**ç—‡çŠ¶**:
```
âŒ LLMæœåŠ¡é”™è¯¯: 504 - Gateway Timeout
```

**æ’æŸ¥æ­¥éª¤**:
```bash
# 1. æ£€æŸ¥LLMæœåŠ¡çŠ¶æ€
docker logs backend-llm-service-1

# 2. æ£€æŸ¥æ¨¡å‹é…ç½®
curl http://llm-service:8004/api/v1/models

# 3. æ£€æŸ¥APIå¯†é’¥
echo $DEEPSEEK_API_KEY
```

**è§£å†³æ–¹æ¡ˆ**:
- å¢åŠ è¶…æ—¶æ—¶é—´é…ç½®
- æ£€æŸ¥APIå¯†é’¥æœ‰æ•ˆæ€§
- éªŒè¯æ¨¡å‹å¯ç”¨æ€§

#### **3. å›¾æ‰§è¡Œå¡ä½**

**ç—‡çŠ¶**:
```
ğŸ”„ æ‰§è¡Œå¤šå¤´ç ”ç©¶å‘˜èŠ‚ç‚¹
(é•¿æ—¶é—´æ— å“åº”)
```

**æ’æŸ¥æ­¥éª¤**:
```bash
# 1. æ£€æŸ¥å›¾çŠ¶æ€
curl http://analysis-engine:8002/api/v1/analysis/status/{analysis_id}

# 2. æ£€æŸ¥AgentçŠ¶æ€
curl http://agent-service:8005/api/v1/agents/status

# 3. æ£€æŸ¥èµ„æºä½¿ç”¨
docker stats
```

**è§£å†³æ–¹æ¡ˆ**:
- æ£€æŸ¥æ¡ä»¶é€»è¾‘æ˜¯å¦æ­£ç¡®
- éªŒè¯Agentå“åº”æ ¼å¼
- å¢åŠ è¶…æ—¶å’Œé‡è¯•æœºåˆ¶

#### **4. è¾©è®ºæ— æ³•ç»“æŸ**

**ç—‡çŠ¶**:
```
ğŸ—£ï¸ è¾©è®ºç¬¬10è½®å¼€å§‹
(è¶…è¿‡é¢„æœŸè½®æ•°)
```

**æ’æŸ¥æ­¥éª¤**:
```python
# æ£€æŸ¥æ¡ä»¶é€»è¾‘
def should_continue_debate(self, state):
    print(f"å½“å‰è½®æ•°: {self.debate_state['count']}")
    print(f"æœ€å¤§è½®æ•°: {self.max_debate_rounds}")
    print(f"å½“å‰å‘è¨€è€…: {self.debate_state['current_speaker']}")
```

**è§£å†³æ–¹æ¡ˆ**:
- æ£€æŸ¥è¾©è®ºè½®æ•°è®¡ç®—é€»è¾‘
- éªŒè¯çŠ¶æ€æ›´æ–°æœºåˆ¶
- æ·»åŠ å¼ºåˆ¶ç»ˆæ­¢æ¡ä»¶

### **ç›‘æ§å’Œæ—¥å¿—**

#### **å…³é”®æ—¥å¿—ä½ç½®**
```
Analysis Engine: backend/analysis-engine/logs/
Agent Service: backend/agent-service/logs/
LLM Service: backend/llm-service/logs/
```

#### **å…³é”®ç›‘æ§æŒ‡æ ‡**
```python
# æœåŠ¡å¥åº·çŠ¶æ€
GET /health

# å›¾æ‰§è¡ŒçŠ¶æ€
GET /api/v1/analysis/status/{analysis_id}

# AgentçŠ¶æ€
GET /api/v1/agents/status

# LLMä½¿ç”¨ç»Ÿè®¡
GET /api/v1/usage/stats
```

## ğŸ”„ **å¼€å‘è°ƒè¯•æŠ€å·§**

### **æœ¬åœ°å¼€å‘ç¯å¢ƒ**

#### **å¯åŠ¨é¡ºåº**
```bash
# 1. å¯åŠ¨åŸºç¡€æœåŠ¡
docker-compose up -d redis mongodb

# 2. å¯åŠ¨LLMæœåŠ¡
cd llm-service && python -m uvicorn app.main:app --port 8004

# 3. å¯åŠ¨AgentæœåŠ¡
cd agent-service && python -m uvicorn app.main:app --port 8005

# 4. å¯åŠ¨Analysis Engine
cd analysis-engine && python -m uvicorn app.main:app --port 8002
```

#### **è°ƒè¯•æŠ€å·§**
```python
# 1. å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.getLogger().setLevel(logging.DEBUG)

# 2. æ·»åŠ è°ƒè¯•æ–­ç‚¹
import pdb; pdb.set_trace()

# 3. æ¨¡æ‹ŸæœåŠ¡è°ƒç”¨
async def test_agent_call():
    async with aiohttp.ClientSession() as session:
        response = await session.post(
            "http://localhost:8005/api/v1/agents/market_analyst/analyze",
            json={"symbol": "AAPL", "context": {}}
        )
        print(await response.json())
```

### **æ€§èƒ½ä¼˜åŒ–å»ºè®®**

#### **è¿æ¥æ± ä¼˜åŒ–**
```python
# ä½¿ç”¨è¿æ¥æ± 
connector = aiohttp.TCPConnector(
    limit=100,           # æ€»è¿æ¥æ•°é™åˆ¶
    limit_per_host=30,   # æ¯ä¸ªä¸»æœºè¿æ¥æ•°é™åˆ¶
    keepalive_timeout=30 # ä¿æŒè¿æ¥æ—¶é—´
)
session = aiohttp.ClientSession(connector=connector)
```

#### **å¹¶å‘æ§åˆ¶**
```python
# ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
semaphore = asyncio.Semaphore(10)

async def call_with_limit():
    async with semaphore:
        return await make_api_call()
```

## ğŸ“š **ç›¸å…³æ–‡æ¡£**

- [å›¾æ¶æ„è®¾è®¡æ–‡æ¡£](graph_architecture_design.md)
- [å›¾å®ç°æŠ€æœ¯è§„èŒƒ](../technical/graph_implementation_spec.md)
- [APIå‚è€ƒæ–‡æ¡£](../api/)
- [éƒ¨ç½²æŒ‡å—](../deployment/deployment-guide.md)
- [æ•…éšœæ’é™¤æŒ‡å—](../troubleshooting/)

## ğŸ” **æœåŠ¡ä¾èµ–å…³ç³»è¯¦è§£**

### **ä¾èµ–å±‚æ¬¡å›¾**

```
Level 1: åŸºç¡€æœåŠ¡
â”œâ”€â”€ Redis (ç¼“å­˜)
â”œâ”€â”€ MongoDB (æ•°æ®å­˜å‚¨)
â””â”€â”€ Data Service (æ•°æ®æ¥å£)

Level 2: æ ¸å¿ƒæœåŠ¡
â”œâ”€â”€ LLM Service (æ¨¡å‹è°ƒç”¨)
â””â”€â”€ Memory Service (çŠ¶æ€ç®¡ç†)

Level 3: ä¸šåŠ¡æœåŠ¡
â”œâ”€â”€ Agent Service (æ™ºèƒ½ä½“ç®¡ç†)
â””â”€â”€ Task Scheduler (ä»»åŠ¡è°ƒåº¦)

Level 4: åº”ç”¨æœåŠ¡
â”œâ”€â”€ Analysis Engine (åˆ†æå¼•æ“)
â””â”€â”€ API Gateway (ç»Ÿä¸€å…¥å£)
```

### **å¯åŠ¨ä¾èµ–é¡ºåº**

#### **å¿…é¡»çš„å¯åŠ¨é¡ºåº**
```bash
# 1. åŸºç¡€è®¾æ–½ (å¿…é¡»æœ€å…ˆå¯åŠ¨)
docker-compose up -d redis mongodb

# 2. æ•°æ®æœåŠ¡ (ä¸ºå…¶ä»–æœåŠ¡æä¾›æ•°æ®)
docker-compose up -d data-service

# 3. LLMæœåŠ¡ (AgentæœåŠ¡ä¾èµ–)
docker-compose up -d llm-service

# 4. å†…å­˜æœåŠ¡ (çŠ¶æ€ç®¡ç†)
docker-compose up -d memory-service

# 5. AgentæœåŠ¡ (Analysis Engineä¾èµ–)
docker-compose up -d agent-service

# 6. åˆ†æå¼•æ“ (ä¸šåŠ¡æ ¸å¿ƒ)
docker-compose up -d analysis-engine

# 7. APIç½‘å…³ (å¯¹å¤–æ¥å£)
docker-compose up -d api-gateway
```

#### **å¥åº·æ£€æŸ¥éªŒè¯**
```bash
# éªŒè¯æœåŠ¡å¯åŠ¨é¡ºåº
curl http://localhost:8003/health  # Data Service
curl http://localhost:8004/health  # LLM Service
curl http://localhost:8006/health  # Memory Service
curl http://localhost:8005/health  # Agent Service
curl http://localhost:8002/health  # Analysis Engine
curl http://localhost:8001/health  # API Gateway
```

## ğŸ”„ **æ•°æ®æµå‘åˆ†æ**

### **è¯·æ±‚æ•°æ®æµ**

```
ç”¨æˆ·è¯·æ±‚ â†’ API Gateway â†’ Analysis Engine â†’ Agent Service â†’ LLM Service
    â†“           â†“              â†“              â†“           â†“
  è·¯ç”±åˆ†å‘   å›¾å¼•æ“è°ƒåº¦    æ™ºèƒ½ä½“ç®¡ç†    Agentæ‰§è¡Œ   æ¨¡å‹æ¨ç†
    â†“           â†“              â†“              â†“           â†“
  å‚æ•°éªŒè¯   çŠ¶æ€ç®¡ç†      ä»»åŠ¡åˆ†é…      æç¤ºæ„å»º    ç»“æœç”Ÿæˆ
```

### **çŠ¶æ€æ•°æ®æµ**

```
GraphState â†’ AgentNodes â†’ Agent Service â†’ TaskContext â†’ LLM Service
    â†“            â†“            â†“             â†“            â†“
  å›¾çŠ¶æ€     èŠ‚ç‚¹çŠ¶æ€    æ™ºèƒ½ä½“çŠ¶æ€    ä»»åŠ¡ä¸Šä¸‹æ–‡   æ¨¡å‹çŠ¶æ€
    â†“            â†“            â†“             â†“            â†“
Memory Service â† State Manager â† Agent Manager â† Task Manager â† Usage Tracker
```

### **ç»“æœæ•°æ®æµ**

```
LLM Response â†’ Agent Result â†’ Node Result â†’ Graph Result â†’ API Response
     â†“             â†“            â†“            â†“            â†“
  æ¨¡å‹è¾“å‡º     æ™ºèƒ½ä½“ç»“æœ   èŠ‚ç‚¹è¾“å‡º     å›¾æ‰§è¡Œç»“æœ   æœ€ç»ˆå“åº”
     â†“             â†“            â†“            â†“            â†“
  æ ¼å¼åŒ–       ç»“æœå¤„ç†     çŠ¶æ€æ›´æ–°     ç»“æœæ±‡æ€»     å“åº”æ ¼å¼åŒ–
```

## ğŸ› ï¸ **å¼€å‘ç¯å¢ƒé…ç½®**

### **Docker Composeé…ç½®**

#### **å¼€å‘ç¯å¢ƒé…ç½®æ–‡ä»¶**
```yaml
# docker-compose.dev.yml
version: '3.8'
services:
  analysis-engine:
    build: ./analysis-engine
    ports:
      - "8002:8002"
    environment:
      - AGENT_SERVICE_URL=http://agent-service:8005
      - LLM_SERVICE_URL=http://llm-service:8004
      - DATA_SERVICE_URL=http://data-service:8003
      - MEMORY_SERVICE_URL=http://memory-service:8006
    depends_on:
      - agent-service
      - data-service
    volumes:
      - ./analysis-engine:/app
    command: uvicorn app.main:app --host 0.0.0.0 --port 8002 --reload

  agent-service:
    build: ./agent-service
    ports:
      - "8005:8005"
    environment:
      - LLM_SERVICE_URL=http://llm-service:8004
      - MEMORY_SERVICE_URL=http://memory-service:8006
    depends_on:
      - llm-service
      - memory-service
    volumes:
      - ./agent-service:/app
    command: uvicorn app.main:app --host 0.0.0.0 --port 8005 --reload

  llm-service:
    build: ./llm-service
    ports:
      - "8004:8004"
    environment:
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./llm-service:/app
    command: uvicorn app.main:app --host 0.0.0.0 --port 8004 --reload
```

### **æœ¬åœ°å¼€å‘è„šæœ¬**

#### **å¯åŠ¨è„šæœ¬**
```bash
#!/bin/bash
# scripts/start-dev.sh

echo "ğŸš€ å¯åŠ¨Backendå¼€å‘ç¯å¢ƒ..."

# æ£€æŸ¥ç¯å¢ƒå˜é‡
if [ -z "$DEEPSEEK_API_KEY" ]; then
    echo "âŒ è¯·è®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡"
    exit 1
fi

# å¯åŠ¨åŸºç¡€æœåŠ¡
echo "ğŸ“¦ å¯åŠ¨åŸºç¡€æœåŠ¡..."
docker-compose -f docker-compose.dev.yml up -d redis mongodb

# ç­‰å¾…åŸºç¡€æœåŠ¡å¯åŠ¨
sleep 5

# å¯åŠ¨ä¸šåŠ¡æœåŠ¡
echo "ğŸ”§ å¯åŠ¨ä¸šåŠ¡æœåŠ¡..."
docker-compose -f docker-compose.dev.yml up -d data-service llm-service memory-service

# ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 10

# å¯åŠ¨æ ¸å¿ƒæœåŠ¡
echo "ğŸ¯ å¯åŠ¨æ ¸å¿ƒæœåŠ¡..."
docker-compose -f docker-compose.dev.yml up -d agent-service analysis-engine

# ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 10

# å¯åŠ¨APIç½‘å…³
echo "ğŸŒ å¯åŠ¨APIç½‘å…³..."
docker-compose -f docker-compose.dev.yml up -d api-gateway

echo "âœ… å¼€å‘ç¯å¢ƒå¯åŠ¨å®Œæˆ!"
echo "ğŸ“Š æœåŠ¡çŠ¶æ€æ£€æŸ¥..."

# å¥åº·æ£€æŸ¥
services=("data-service:8003" "llm-service:8004" "agent-service:8005" "analysis-engine:8002" "api-gateway:8001")
for service in "${services[@]}"; do
    name=$(echo $service | cut -d: -f1)
    port=$(echo $service | cut -d: -f2)

    if curl -s http://localhost:$port/health > /dev/null; then
        echo "âœ… $name (ç«¯å£ $port) - å¥åº·"
    else
        echo "âŒ $name (ç«¯å£ $port) - å¼‚å¸¸"
    fi
done
```

#### **åœæ­¢è„šæœ¬**
```bash
#!/bin/bash
# scripts/stop-dev.sh

echo "ğŸ›‘ åœæ­¢Backendå¼€å‘ç¯å¢ƒ..."

# åœæ­¢æ‰€æœ‰æœåŠ¡
docker-compose -f docker-compose.dev.yml down

# æ¸…ç†èµ„æº
docker system prune -f

echo "âœ… å¼€å‘ç¯å¢ƒå·²åœæ­¢"
```

### **è°ƒè¯•é…ç½®**

#### **VSCodeè°ƒè¯•é…ç½®**
```json
// .vscode/launch.json
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Debug Analysis Engine",
            "type": "python",
            "request": "launch",
            "program": "${workspaceFolder}/analysis-engine/app/main.py",
            "console": "integratedTerminal",
            "env": {
                "AGENT_SERVICE_URL": "http://localhost:8005",
                "LLM_SERVICE_URL": "http://localhost:8004",
                "DATA_SERVICE_URL": "http://localhost:8003"
            },
            "args": ["--host", "0.0.0.0", "--port", "8002"]
        },
        {
            "name": "Debug Agent Service",
            "type": "python",
            "request": "launch",
            "program": "${workspaceFolder}/agent-service/app/main.py",
            "console": "integratedTerminal",
            "env": {
                "LLM_SERVICE_URL": "http://localhost:8004",
                "MEMORY_SERVICE_URL": "http://localhost:8006"
            },
            "args": ["--host", "0.0.0.0", "--port", "8005"]
        },
        {
            "name": "Debug LLM Service",
            "type": "python",
            "request": "launch",
            "program": "${workspaceFolder}/llm-service/app/main.py",
            "console": "integratedTerminal",
            "env": {
                "DEEPSEEK_API_KEY": "${env:DEEPSEEK_API_KEY}"
            },
            "args": ["--host", "0.0.0.0", "--port", "8004"]
        }
    ]
}
```

## ğŸ“ˆ **æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–**

### **å…³é”®æ€§èƒ½æŒ‡æ ‡**

#### **æœåŠ¡çº§åˆ«æŒ‡æ ‡**
```python
# å“åº”æ—¶é—´ç›‘æ§
response_time_metrics = {
    "analysis_engine_response_time": "åˆ†æå¼•æ“å“åº”æ—¶é—´",
    "agent_service_response_time": "æ™ºèƒ½ä½“æœåŠ¡å“åº”æ—¶é—´",
    "llm_service_response_time": "LLMæœåŠ¡å“åº”æ—¶é—´",
    "graph_execution_time": "å›¾æ‰§è¡Œæ€»æ—¶é—´",
    "debate_round_time": "å•è½®è¾©è®ºæ—¶é—´"
}

# ååé‡ç›‘æ§
throughput_metrics = {
    "requests_per_second": "æ¯ç§’è¯·æ±‚æ•°",
    "concurrent_analyses": "å¹¶å‘åˆ†ææ•°",
    "agent_utilization": "æ™ºèƒ½ä½“åˆ©ç”¨ç‡",
    "llm_calls_per_minute": "æ¯åˆ†é’ŸLLMè°ƒç”¨æ•°"
}

# é”™è¯¯ç‡ç›‘æ§
error_metrics = {
    "service_error_rate": "æœåŠ¡é”™è¯¯ç‡",
    "timeout_rate": "è¶…æ—¶ç‡",
    "llm_failure_rate": "LLMè°ƒç”¨å¤±è´¥ç‡",
    "graph_failure_rate": "å›¾æ‰§è¡Œå¤±è´¥ç‡"
}
```

#### **ä¸šåŠ¡çº§åˆ«æŒ‡æ ‡**
```python
# åˆ†æè´¨é‡æŒ‡æ ‡
quality_metrics = {
    "debate_rounds_avg": "å¹³å‡è¾©è®ºè½®æ•°",
    "consensus_rate": "å…±è¯†è¾¾æˆç‡",
    "analysis_completion_rate": "åˆ†æå®Œæˆç‡",
    "recommendation_confidence": "æ¨èç½®ä¿¡åº¦"
}
```

### **æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**

#### **è¿æ¥æ± ä¼˜åŒ–**
```python
# Analysis Engineä¸­çš„è¿æ¥æ± é…ç½®
class ServiceConnections:
    def __init__(self):
        self.agent_service_pool = aiohttp.TCPConnector(
            limit=50,              # æ€»è¿æ¥æ•°
            limit_per_host=20,     # æ¯ä¸»æœºè¿æ¥æ•°
            keepalive_timeout=60,  # ä¿æŒè¿æ¥æ—¶é—´
            enable_cleanup_closed=True
        )

        self.session = aiohttp.ClientSession(
            connector=self.agent_service_pool,
            timeout=aiohttp.ClientTimeout(total=300)
        )
```

#### **ç¼“å­˜ç­–ç•¥**
```python
# LLMç»“æœç¼“å­˜
class LLMCache:
    def __init__(self):
        self.redis_client = redis.Redis()
        self.cache_ttl = 3600  # 1å°æ—¶

    async def get_cached_response(self, prompt_hash: str):
        return await self.redis_client.get(f"llm_cache:{prompt_hash}")

    async def cache_response(self, prompt_hash: str, response: str):
        await self.redis_client.setex(
            f"llm_cache:{prompt_hash}",
            self.cache_ttl,
            response
        )
```

#### **è´Ÿè½½å‡è¡¡**
```python
# Agentè´Ÿè½½å‡è¡¡
class AgentLoadBalancer:
    def select_agent(self, available_agents: List[BaseAgent]) -> BaseAgent:
        # åŸºäºå½“å‰è´Ÿè½½é€‰æ‹©æœ€ä¼˜Agent
        return min(available_agents, key=lambda a: a.current_load)
```

---

*æœ¬æ–‡æ¡£æä¾›Backendæ ¸å¿ƒæœåŠ¡å…³ç³»çš„å®Œæ•´è¯´æ˜ï¼Œä¸ºå¼€å‘å’Œè¿ç»´æä¾›å‚è€ƒã€‚å¦‚æœ‰é—®é¢˜è¯·å‚è€ƒç›¸å…³æ–‡æ¡£æˆ–è”ç³»å¼€å‘å›¢é˜Ÿã€‚*
