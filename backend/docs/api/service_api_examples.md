# BackendæœåŠ¡APIè°ƒç”¨ç¤ºä¾‹

## ğŸ“‹ **æ¦‚è¿°**

æœ¬æ–‡æ¡£æä¾›Backendå„æœåŠ¡é—´APIè°ƒç”¨çš„è¯¦ç»†ç¤ºä¾‹ï¼Œå¸®åŠ©å¼€å‘è€…ç†è§£æœåŠ¡é—´çš„äº¤äº’æ–¹å¼ã€‚

## ğŸ”„ **å®Œæ•´è°ƒç”¨é“¾ç¤ºä¾‹**

### **è‚¡ç¥¨åˆ†æå®Œæ•´æµç¨‹**

#### **1. ç”¨æˆ·å‘èµ·åˆ†æè¯·æ±‚**
```bash
# é€šè¿‡API Gatewayå‘èµ·è¯·æ±‚
curl -X POST http://localhost:8001/api/v1/analysis/comprehensive \
  -H "Content-Type: application/json" \
  -d '{
    "symbol": "000001",
    "company_name": "å¹³å®‰é“¶è¡Œ",
    "market": "CN",
    "analysis_type": "comprehensive"
  }'

# å“åº”
{
  "success": true,
  "data": {
    "analysis_id": "uuid-12345",
    "status": "started",
    "message": "åˆ†æå·²å¯åŠ¨"
  }
}
```

#### **2. Analysis Engineå†…éƒ¨å›¾æ‰§è¡Œ**
```python
# Analysis Engineå†…éƒ¨è°ƒç”¨
# ä½ç½®: backend/analysis-engine/app/graphs/trading_graph.py

async def analyze_stock(self, symbol: str, analysis_date: str = None):
    # åˆ›å»ºåˆå§‹çŠ¶æ€
    initial_state = self._create_initial_state(symbol, analysis_date)
    
    # æ‰§è¡Œå›¾
    final_state = await self.compiled_graph.ainvoke(initial_state)
    
    return self._process_final_state(final_state)
```

#### **3. å›¾èŠ‚ç‚¹è°ƒç”¨Agent Service**
```python
# å¸‚åœºåˆ†æå¸ˆèŠ‚ç‚¹è°ƒç”¨
# ä½ç½®: backend/analysis-engine/app/graphs/agent_nodes.py

async def market_analyst_node(self, state: GraphState) -> GraphState:
    # è°ƒç”¨Agent Service
    result = await self._call_agent_service(
        "market_analyst",
        "analyze", 
        {
            "symbol": state["symbol"],
            "analysis_type": "technical",
            "context": {
                "current_date": state["current_date"],
                "existing_data": state.get("stock_data")
            }
        }
    )
    
    # HTTPè°ƒç”¨è¯¦æƒ…
    url = f"{self.agent_service_url}/api/v1/agents/market_analyst/analyze"
    # POST http://agent-service:8005/api/v1/agents/market_analyst/analyze
```

#### **4. Agent Serviceå¤„ç†è¯·æ±‚**
```python
# Agent Service APIå¤„ç†
# ä½ç½®: backend/agent-service/app/api/agents_api.py

@router.post("/agents/{agent_type}/{action}")
async def execute_agent_action(
    agent_type: str,
    action: str,
    request: Dict[str, Any]
):
    # è·å–Agent Manager
    agent_manager = get_agent_manager()
    
    # æ‰§è¡Œä»»åŠ¡
    result = await agent_manager.execute_task(
        agent_type=AgentType(agent_type),
        task_type=TaskType(action),
        context=TaskContext(**request)
    )
    
    return {"success": True, "data": result}
```

#### **5. Agentè°ƒç”¨LLM Service**
```python
# å¸‚åœºåˆ†æå¸ˆè°ƒç”¨LLM
# ä½ç½®: backend/agent-service/app/agents/analysts/market_analyst.py

async def _generate_analysis_report(self, context, market_data, ...):
    prompt = self.analysis_template.format(
        symbol=context.symbol,
        company_name=context.company_name,
        market_data=market_data,
        # ...
    )
    
    # è°ƒç”¨LLM Service
    response = await self.llm_client.chat_completion(
        messages=[{"role": "user", "content": prompt}],
        model="deepseek-chat",
        temperature=0.1
    )
    
    # HTTPè°ƒç”¨è¯¦æƒ…
    # POST http://llm-service:8004/api/v1/chat/completions
```

#### **6. LLM Serviceå¤„ç†è¯·æ±‚**
```python
# LLM Service APIå¤„ç†
# ä½ç½®: backend/llm-service/app/main.py

@app.post("/api/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    # æ¨¡å‹è·¯ç”±
    adapter = model_router.get_adapter(request.model)
    
    # è°ƒç”¨é€‚é…å™¨
    result = await adapter.chat_completion(
        messages=request.messages,
        max_tokens=request.max_tokens,
        temperature=request.temperature
    )
    
    return ChatCompletionResponse(**result)
```

## ğŸ” **å„æœåŠ¡APIè¯¦ç»†ç¤ºä¾‹**

### **Analysis Engine API**

#### **å¯åŠ¨åˆ†æ**
```bash
POST http://localhost:8002/api/v1/analysis/graph/analyze
Content-Type: application/json

{
  "symbol": "000001",
  "analysis_type": "comprehensive",
  "config": {
    "max_debate_rounds": 3,
    "max_risk_rounds": 2,
    "selected_analysts": ["market", "fundamentals", "news"]
  }
}

# å“åº”
{
  "success": true,
  "data": {
    "analysis_id": "uuid-12345",
    "status": "running",
    "current_step": "market_analyst",
    "progress": {
      "completed_steps": 1,
      "total_steps": 12,
      "percentage": 8.3
    }
  }
}
```

#### **æŸ¥è¯¢åˆ†æçŠ¶æ€**
```bash
GET http://localhost:8002/api/v1/analysis/status/uuid-12345

# å“åº”
{
  "success": true,
  "data": {
    "analysis_id": "uuid-12345",
    "status": "running",
    "current_step": "bull_researcher",
    "progress": {
      "completed_steps": 6,
      "total_steps": 12,
      "percentage": 50.0
    },
    "debate_status": {
      "current_round": 2,
      "max_rounds": 3,
      "current_speaker": "bull_researcher"
    },
    "errors": [],
    "warnings": []
  }
}
```

#### **è·å–åˆ†æç»“æœ**
```bash
GET http://localhost:8002/api/v1/analysis/result/uuid-12345

# å“åº”
{
  "success": true,
  "data": {
    "analysis_id": "uuid-12345",
    "symbol": "000001",
    "status": "completed",
    "result": {
      "final_recommendation": {
        "action": "buy",
        "confidence": 0.75,
        "target_price": 15.50,
        "reasoning": "åŸºäºå¤šè½®è¾©è®ºå’Œé£é™©åˆ†æçš„ç»¼åˆå»ºè®®"
      },
      "investment_plan": "å»ºè®®åˆ†æ‰¹ä¹°å…¥ï¼Œæ§åˆ¶ä»“ä½åœ¨5%ä»¥å†…",
      "risk_assessment": {
        "risk_level": "medium",
        "risk_score": 0.6,
        "key_risks": ["å¸‚åœºæ³¢åŠ¨", "è¡Œä¸šæ”¿ç­–å˜åŒ–"]
      },
      "reports": {
        "fundamentals": "åŸºæœ¬é¢åˆ†ææŠ¥å‘Š...",
        "technical": "æŠ€æœ¯åˆ†ææŠ¥å‘Š...",
        "news": "æ–°é—»åˆ†ææŠ¥å‘Š...",
        "sentiment": "æƒ…æ„Ÿåˆ†ææŠ¥å‘Š..."
      },
      "debate_summary": {
        "total_rounds": 3,
        "consensus_reached": true,
        "final_stance": "moderately_bullish"
      }
    }
  }
}
```

### **Agent Service API**

#### **æ‰§è¡Œæ™ºèƒ½ä½“ä»»åŠ¡**
```bash
POST http://localhost:8005/api/v1/agents/market_analyst/analyze
Content-Type: application/json

{
  "symbol": "000001",
  "analysis_type": "technical",
  "context": {
    "current_date": "2024-01-22",
    "market": "CN",
    "existing_data": {}
  }
}

# å“åº”
{
  "success": true,
  "data": {
    "agent_id": "market_analyst_001",
    "agent_type": "market_analyst",
    "task_id": "task_12345",
    "status": "completed",
    "result": {
      "analysis": "æŠ€æœ¯åˆ†ææŠ¥å‘Šå†…å®¹...",
      "market_data": {
        "current_price": 14.25,
        "volume": 1234567,
        "technical_indicators": {
          "rsi": 65.5,
          "macd": 0.15,
          "ma20": 14.10
        }
      },
      "confidence_score": 0.8
    },
    "execution_time": 15.5,
    "timestamp": "2024-01-22T10:30:00Z"
  }
}
```

#### **å¯åŠ¨è¾©è®º**
```bash
POST http://localhost:8005/api/v1/debate/start
Content-Type: application/json

{
  "topic": "000001æŠ•èµ„å†³ç­–",
  "participants": ["bull_researcher", "bear_researcher"],
  "context": {
    "symbol": "000001",
    "company_name": "å¹³å®‰é“¶è¡Œ",
    "market": "CN",
    "analysis_date": "2024-01-22"
  },
  "rules": {
    "max_rounds": 3,
    "timeout_per_round": 120
  }
}

# å“åº”
{
  "success": true,
  "data": {
    "debate_id": "debate_12345",
    "status": "running",
    "topic": "000001æŠ•èµ„å†³ç­–",
    "participants": ["bull_researcher", "bear_researcher"],
    "current_round": 1,
    "max_rounds": 3
  }
}
```

#### **æŸ¥è¯¢è¾©è®ºçŠ¶æ€**
```bash
GET http://localhost:8005/api/v1/debate/debate_12345/status

# å“åº”
{
  "success": true,
  "data": {
    "debate_id": "debate_12345",
    "status": "running",
    "current_round": 2,
    "current_speaker": "bear_researcher",
    "rounds": [
      {
        "round_number": 1,
        "bull_argument": "å¤šå¤´è§‚ç‚¹...",
        "bear_argument": "ç©ºå¤´è§‚ç‚¹...",
        "timestamp": "2024-01-22T10:35:00Z"
      }
    ],
    "consensus": null
  }
}
```

### **LLM Service API**

#### **èŠå¤©å®Œæˆ**
```bash
POST http://localhost:8004/api/v1/chat/completions
Content-Type: application/json

{
  "model": "deepseek-chat",
  "messages": [
    {
      "role": "user", 
      "content": "è¯·åˆ†æå¹³å®‰é“¶è¡Œ(000001)çš„æŠ€æœ¯æŒ‡æ ‡"
    }
  ],
  "max_tokens": 1500,
  "temperature": 0.1
}

# å“åº”
{
  "success": true,
  "data": {
    "content": "åŸºäºæŠ€æœ¯æŒ‡æ ‡åˆ†æï¼Œå¹³å®‰é“¶è¡Œå½“å‰...",
    "usage": {
      "prompt_tokens": 25,
      "completion_tokens": 150,
      "total_tokens": 175
    },
    "model": "deepseek-chat",
    "finish_reason": "stop"
  }
}
```

#### **è·å–å¯ç”¨æ¨¡å‹**
```bash
GET http://localhost:8004/api/v1/models

# å“åº”
{
  "success": true,
  "data": {
    "models": [
      {
        "id": "deepseek-chat",
        "provider": "DeepSeek",
        "status": "available",
        "context_length": 128000,
        "supports_tools": true
      },
      {
        "id": "gpt-4",
        "provider": "OpenAI", 
        "status": "available",
        "context_length": 8192,
        "supports_tools": true
      }
    ]
  }
}
```

#### **ä½¿ç”¨ç»Ÿè®¡**
```bash
GET http://localhost:8004/api/v1/usage/stats

# å“åº”
{
  "success": true,
  "data": {
    "total_requests": 1250,
    "total_tokens": 125000,
    "requests_by_model": {
      "deepseek-chat": 1000,
      "gpt-4": 250
    },
    "tokens_by_model": {
      "deepseek-chat": 100000,
      "gpt-4": 25000
    },
    "average_response_time": 2.5,
    "error_rate": 0.02
  }
}
```

## ğŸ”§ **è°ƒè¯•å’Œæµ‹è¯•å·¥å…·**

### **æœåŠ¡è¿é€šæ€§æµ‹è¯•**
```bash
#!/bin/bash
# scripts/test-connectivity.sh

echo "ğŸ” æµ‹è¯•æœåŠ¡é—´è¿é€šæ€§..."

# æµ‹è¯•Analysis Engine -> Agent Service
echo "ğŸ“Š æµ‹è¯•Analysis Engine -> Agent Service"
docker exec backend-analysis-engine-1 curl -s http://agent-service:8005/health

# æµ‹è¯•Agent Service -> LLM Service  
echo "ğŸ¤– æµ‹è¯•Agent Service -> LLM Service"
docker exec backend-agent-service-1 curl -s http://llm-service:8004/health

# æµ‹è¯•å®Œæ•´è°ƒç”¨é“¾
echo "ğŸ”„ æµ‹è¯•å®Œæ•´è°ƒç”¨é“¾"
curl -X POST http://localhost:8002/api/v1/analysis/test \
  -H "Content-Type: application/json" \
  -d '{"symbol": "TEST", "test_mode": true}'
```

### **æ€§èƒ½æµ‹è¯•è„šæœ¬**
```bash
#!/bin/bash
# scripts/performance-test.sh

echo "âš¡ æ€§èƒ½æµ‹è¯•å¼€å§‹..."

# å¹¶å‘åˆ†ææµ‹è¯•
for i in {1..10}; do
  curl -X POST http://localhost:8002/api/v1/analysis/comprehensive \
    -H "Content-Type: application/json" \
    -d "{\"symbol\": \"TEST$i\", \"test_mode\": true}" &
done

wait
echo "âœ… å¹¶å‘æµ‹è¯•å®Œæˆ"

# å“åº”æ—¶é—´æµ‹è¯•
time curl -X POST http://localhost:8002/api/v1/analysis/comprehensive \
  -H "Content-Type: application/json" \
  -d '{"symbol": "PERF_TEST", "test_mode": true}'
```

### **APIæµ‹è¯•é›†åˆ**
```python
# tests/api_integration_test.py
import asyncio
import aiohttp
import pytest

class TestServiceIntegration:
    
    async def test_full_analysis_flow(self):
        """æµ‹è¯•å®Œæ•´åˆ†ææµç¨‹"""
        async with aiohttp.ClientSession() as session:
            # 1. å¯åŠ¨åˆ†æ
            async with session.post(
                "http://localhost:8002/api/v1/analysis/comprehensive",
                json={"symbol": "TEST001", "test_mode": True}
            ) as resp:
                result = await resp.json()
                analysis_id = result["data"]["analysis_id"]
            
            # 2. ç­‰å¾…å®Œæˆ
            while True:
                async with session.get(
                    f"http://localhost:8002/api/v1/analysis/status/{analysis_id}"
                ) as resp:
                    status = await resp.json()
                    if status["data"]["status"] == "completed":
                        break
                await asyncio.sleep(1)
            
            # 3. è·å–ç»“æœ
            async with session.get(
                f"http://localhost:8002/api/v1/analysis/result/{analysis_id}"
            ) as resp:
                result = await resp.json()
                assert result["success"] == True
                assert "final_recommendation" in result["data"]["result"]

    async def test_agent_service_direct(self):
        """æµ‹è¯•Agent Serviceç›´æ¥è°ƒç”¨"""
        async with aiohttp.ClientSession() as session:
            async with session.post(
                "http://localhost:8005/api/v1/agents/market_analyst/analyze",
                json={
                    "symbol": "TEST002",
                    "analysis_type": "technical",
                    "context": {"test_mode": True}
                }
            ) as resp:
                result = await resp.json()
                assert result["success"] == True
                assert "analysis" in result["data"]["result"]

    async def test_llm_service_direct(self):
        """æµ‹è¯•LLM Serviceç›´æ¥è°ƒç”¨"""
        async with aiohttp.ClientSession() as session:
            async with session.post(
                "http://localhost:8004/api/v1/chat/completions",
                json={
                    "model": "deepseek-chat",
                    "messages": [{"role": "user", "content": "Hello"}],
                    "max_tokens": 50
                }
            ) as resp:
                result = await resp.json()
                assert result["success"] == True
                assert "content" in result["data"]
```

---

*æœ¬æ–‡æ¡£æä¾›BackendæœåŠ¡APIè°ƒç”¨çš„å®Œæ•´ç¤ºä¾‹ï¼Œå¸®åŠ©å¼€å‘è€…ç†è§£å’Œè°ƒè¯•æœåŠ¡é—´äº¤äº’ã€‚*
