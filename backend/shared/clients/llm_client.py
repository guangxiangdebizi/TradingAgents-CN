"""
LLMæœåŠ¡å®¢æˆ·ç«¯
"""

from typing import Optional, Dict, Any, List
from .base import BaseServiceClient
from ..utils.logger import get_service_logger

logger = get_service_logger("llm-client")


class LLMClient(BaseServiceClient):
    """LLMæœåŠ¡å®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: Optional[str] = None):
        super().__init__("llm-service", base_url)
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: str = "deepseek-chat",
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        èŠå¤©å®Œæˆæ¥å£
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"role": "user", "content": "..."}]
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            LLMå“åº”ç»“æœ
        """
        try:
            data = {
                "messages": messages,
                "model": model,
                "temperature": temperature,
                **kwargs
            }

            if max_tokens:
                data["max_tokens"] = max_tokens

            self.logger.info(f"ğŸ” LLMå®¢æˆ·ç«¯è¯·æ±‚: model={model}, messages_count={len(messages)}")
            self.logger.info(f"ğŸ” LLMå®¢æˆ·ç«¯æ¶ˆæ¯: {[{'role': msg['role'], 'content': msg['content'][:200] + '...' if len(msg['content']) > 200 else msg['content']} for msg in messages]}")
            self.logger.info(f"ğŸ” LLMå®¢æˆ·ç«¯è¯·æ±‚æ•°æ®: {data}")
            self.logger.info(f"ğŸ” LLMå®¢æˆ·ç«¯è¯·æ±‚URL: {self.base_url}/api/v1/chat/completions")
            self.logger.info(f"ğŸ” LLMå®¢æˆ·ç«¯base_url: {self.base_url}")
            self.logger.info(f"ğŸ” LLMå®¢æˆ·ç«¯å®Œæ•´è¯·æ±‚è·¯å¾„: /api/v1/chat/completions")

            response = await self.post("/api/v1/chat/completions", data)

            self.logger.info(f"ğŸ” LLMå®¢æˆ·ç«¯å“åº”: {response}")
            return response
            
        except Exception as e:
            # åˆ¤æ–­æ˜¯å¦ä¸ºè¿æ¥é”™è¯¯æˆ–è¶…æ—¶é”™è¯¯
            if "connection" in str(e).lower() or "timeout" in str(e).lower() or "failed" in str(e).lower():
                self.logger.critical(f"ğŸš¨ ä¸¥é‡å‘Šè­¦: LLM Serviceä¸å¯è¾¾ - æ— æ³•å®Œæˆå¯¹è¯")
                self.logger.critical(f"ğŸš¨ è¯·æ£€æŸ¥LLM Serviceæ˜¯å¦å¯åŠ¨: {self.base_url}")
                self.logger.critical(f"ğŸš¨ é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            else:
                self.logger.error(f"Chat completion failed: {e}")
            raise
    
    async def analyze_text(
        self,
        text: str,
        analysis_type: str = "sentiment",
        model: str = "deepseek-chat",
        **kwargs
    ) -> Dict[str, Any]:
        """
        æ–‡æœ¬åˆ†ææ¥å£
        
        Args:
            text: è¦åˆ†æçš„æ–‡æœ¬
            analysis_type: åˆ†æç±»å‹ (sentiment, summary, keywordsç­‰)
            model: æ¨¡å‹åç§°
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            åˆ†æç»“æœ
        """
        try:
            data = {
                "text": text,
                "analysis_type": analysis_type,
                "model": model,
                **kwargs
            }
            
            self.logger.debug(f"Text analysis request: type={analysis_type}, text_length={len(text)}")
            response = await self.post("/api/v1/analyze", data)
            
            self.logger.debug(f"Text analysis response: {response.get('success', False)}")
            return response
            
        except Exception as e:
            self.logger.error(f"Text analysis failed: {e}")
            raise
    
    async def generate_report(
        self,
        data: Dict[str, Any],
        report_type: str = "analysis",
        model: str = "deepseek-chat",
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç”ŸæˆæŠ¥å‘Šæ¥å£
        
        Args:
            data: æŠ¥å‘Šæ•°æ®
            report_type: æŠ¥å‘Šç±»å‹
            model: æ¨¡å‹åç§°
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            ç”Ÿæˆçš„æŠ¥å‘Š
        """
        try:
            request_data = {
                "data": data,
                "report_type": report_type,
                "model": model,
                **kwargs
            }
            
            self.logger.debug(f"Report generation request: type={report_type}")
            response = await self.post("/api/v1/generate/report", request_data)
            
            self.logger.debug(f"Report generation response: {response.get('success', False)}")
            return response
            
        except Exception as e:
            self.logger.error(f"Report generation failed: {e}")
            raise
    
    async def get_models(self) -> Dict[str, Any]:
        """
        è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        
        Returns:
            æ¨¡å‹åˆ—è¡¨
        """
        try:
            self.logger.debug("Getting available models")
            response = await self.get("/api/v1/models")
            
            self.logger.debug(f"Models response: {len(response.get('data', {}).get('models', []))} models")
            return response
            
        except Exception as e:
            self.logger.error(f"Get models failed: {e}")
            raise
    
    async def get_usage_stats(self) -> Dict[str, Any]:
        """
        è·å–ä½¿ç”¨ç»Ÿè®¡
        
        Returns:
            ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            self.logger.debug("Getting usage statistics")
            response = await self.get("/api/v1/usage/stats")
            
            self.logger.debug("Usage stats retrieved successfully")
            return response
            
        except Exception as e:
            self.logger.error(f"Get usage stats failed: {e}")
            raise


# å…¨å±€LLMå®¢æˆ·ç«¯å®ä¾‹
_llm_client: Optional[LLMClient] = None


def get_llm_client(base_url: Optional[str] = None) -> LLMClient:
    """
    è·å–LLMå®¢æˆ·ç«¯å®ä¾‹
    
    Args:
        base_url: LLMæœåŠ¡çš„åŸºç¡€URL
    
    Returns:
        LLMå®¢æˆ·ç«¯å®ä¾‹
    """
    global _llm_client
    
    if _llm_client is None:
        _llm_client = LLMClient(base_url)
    
    return _llm_client


async def close_llm_client():
    """å…³é—­LLMå®¢æˆ·ç«¯"""
    global _llm_client
    
    if _llm_client:
        await _llm_client.close()
        _llm_client = None
